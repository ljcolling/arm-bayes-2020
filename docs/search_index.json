[
["index.html", "Bayesian Data Analysis Advanced Research Methods | University of Sussex Prerequisites Introduction", " Bayesian Data Analysis Advanced Research Methods | University of Sussex Lincoln Colling 2020-04-05 Prerequisites Each chapter in this book is available at an interactive Juypter notebook on Binder. To launch the Binder instance click on the badge Introduction The aim of this course is to give you an introduction to Bayesian statistics. It is by no means intended to be an exhaustive course, so at the end of it, there will still be a lot for you to learn. However, I do hope that at the end of this workshop you’ll have a better understanding of Bayesian statistics, how it differs from Frequentist approaches, and how to incorporate some Bayesian methods into your research. The course will cover several topics, including the foundations of Frequentist and Bayesian approaches to statistics, how to calculate Bayes factors, Bayesian estimation, and Bayesian regression modelling. "],
["the-p-value.html", "Chapter 1 The p value 1.1 Probability 1.2 Probability and p values 1.3 Interim summary 1.4 A short note on confidence intervals", " Chapter 1 The p value The American Statistical Association (ASA) defines a p value as: the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value (Wasserstein and Lazar 2016) While this is a perfectly acceptable definition, it is maybe a little tricky to understand. The main reason for this is that the definition contains at least one ill-defined concept (“probability”) and one tricky concept (“specified statistical model”). To understand what a p value really is, we’re going to have to unpack both of these ideas. Along the way, we’re going to learn about some other concepts that will also help us understand Frequentist inference. And a good grounding in Frequentist inference will also help us understand the distinction between Frequentist inference and Bayesian inference. 1.1 Probability Most people think of probability as a mathematical concept. In a sense it is, but it is also a deeply philosophical concept. We deploy the word probability in many different kinds of situations, and it’s not clear whether we mean the same thing in each of them. Some examples of where we use the word probability are when we ask questions like: What is the probability of getting heads on repeated tosses of a fair coin? What is the probability that it will rain tomorrow? What is the probability the accused committed the crime? The word probability seemingly refers to different things in each of these situations. For example, we might suggest that the probability of the getting heads is 0.5, where this 0.5 refers to the long-run relative frequency of getting heads. That is, if we were to toss a coin many many times then on around 0.5 (i.e., half) of the tosses the coin would come up heads. We might use a different notion when thinking about the case of somebody accused of a crime. We might say something like, “we are 90% sure” (probability of .9) that the criminal committed the crime. But what does “90% sure” mean. Does it make sense to think of it as the relative frequency? If not, then how else might we think of it? We might, for example, think of it as a credence or a degree of belief that the proposition is true. Or we might think of it as a degree of support. That is, we might say that the available evidence supports the hypothesis that the accused committed the crime with odds of 9-to-1. This list isn’t meant to be exhaustive. The aim is just to highlight that we might sometimes mean different things when we think about probability. It pays to keep this in minds as we move through the course. 1.2 Probability and p values Now that we know that probability can mean different things in different situations, what notion of probability is at play in ASA’s definition of the p value? The common view is to say that it refers to relative frequencies. But relative frequencies of what over repeats of what? the p value refers to the relative frequency of obtaining a statistical summary of the data as large or larger than the observed value over hypothetical repeats of an experiment described by a specified statistical model 1.2.1 Understanding the p through simulation To understand how p values work, let’s start with a little scenario: You’ve been given a device that can be used to find buried treasure. The device has a numbered dial on it, and there is a little arrow that can point at these numbers. The indicator never stays still, but swings around a bit. You don’t know how the device works, except that it behaves differently around treasure compared with when there is no treasure present. How can you use this device to find treasure? This seems like a hard problem. You know very little about the device. You don’t know what it’s meant to do when it finds treasure, and you don’t know what it’s meant to do when there isn’t any treasure. So how do you go about using it to find treasure? 1.2.1.1 Finding treasure The first step in using the device is to get a good description of what it does when there isn’t any treasure around. To do this, you might just take your device somewhere without treasure. You can then just sit and watch the dial. After a long time watching it, you might notice that although the pointer swings around a lot, on average it points at zero. This one bit of information is enough to develop a treasure hunting strategy using this device. The first step in the strategy is deciding how many readings to take on each hunt. Because the pointer swings around a lot, we’ll need to take a couple of readings and then use these to work out an average (which we’ll call \\(\\bar{x}\\)). We’re in a hurry so we’ll take 10 readings on each hunt. Next, we’ll need to scale our average. If our average is 1, then is this close to 0? How about 0.5? Or 5? Or 15? It’s impossible to know because you don’t know how big the average range of the dial’s swings. So your scaling factor should be proportional to the magnitude of the average deviations you’ve observed (we’ll call this scaling factor \\(s_{\\bar{x}}\\)). With this information in hand, we have enough to build a statistical model of our device’s behaviour. To do this, we just go where there is no treasure and perform the following steps: 1) Take 10 readings; 2) work out an average (\\(\\bar{x}\\)); 3) scale it by our scaling factor (\\(s_{\\bar{x}}\\)); write down our scaled measurement (which we’ll call \\(t\\)), and repeat! Once we’ve done this many many times, then we’ll have a nice distribution or statistical model of how our device behaves when there isn’t any treasure. Of course, we don’t have to do this for real. We can just simulate it! Feel free to play around with the simulation, to change the numbers, and to see how this influences our statistical model. # 1 # run this chunk to set up the simulation function set.seed(612) # Set the seed for reproducibility run.exp = function(sample_size, average){ # define function for running and experiment # we don&#39;t know how much the pointer actually swings around, # so lets just pick a random range between 1 and 10! dev = runif(1,1,10); min_possible = 0 -dev; max_possible = 0 + dev # generate a sample of readings this_sample = runif(sample_size, min_possible, max_possible) # make descriptive stats tibble::tibble(sample_mean = mean(this_sample), sample_sd = sd(this_sample), n = sample_size, se = sample_sd / sqrt(sample_size)) } # 2 # run this chunk to actually run the simulations! # set up to run in parallel inParallel = TRUE # set to TRUE for local or FALSE for cloud if(inParallel){ future::plan(multiprocess) no_of_exps = 100000 # Set the number of experiments to simulate map_df &lt;- furrr::future_map_dfr } else { no_of_exps = 1000 } all_exps &lt;- map_df(1:no_of_exps, function(x) run.exp(sample_size = 10, average = 0) %&gt;% dplyr::mutate(i = x)) # run the experiments dplyr::glimpse(all_exps) # view the results ## Observations: 100,000 ## Variables: 5 ## $ sample_mean &lt;dbl&gt; 1.62508653, 0.97752507, -0.68444940, 0.80072471, -0.35928… ## $ sample_sd &lt;dbl&gt; 4.2422385, 2.9602277, 4.9720031, 3.7024161, 1.5273165, 2.… ## $ n &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1… ## $ se &lt;dbl&gt; 1.3415136, 0.9361062, 1.5722854, 1.1708068, 0.4829799, 0.… ## $ i &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… # 3 # run this chunk to view some plots of our simulations all_exps = all_exps %&gt;% mutate(t = (sample_mean * sqrt(n)) / sample_sd) # make a histogram of the unscaled averages all_exps %&gt;% ggplot(aes(x = sample_mean)) + geom_histogram(fill = &quot;seagreen&quot;,bins = 100, na.rm = TRUE) + labs(x = &quot;sample mean&quot;, y = &quot;number of experiments&quot;) + xlim(c(-max(abs(all_exps$sample_mean)) * 1.10, max(abs(all_exps$sample_mean)) * 1.10)) + theme_minimal(12) -&gt; mean_hist # make a histograme of the scaled averages all_exps %&gt;% ggplot(aes(x = t)) + geom_histogram(fill = &quot;seagreen&quot;, bins = 100, na.rm = TRUE) + labs(x = &quot;t stat&quot;, y = &quot;number of experiments&quot;) + xlim(c(-4,4)) + theme_minimal(12) -&gt; t_hist # convert the histogram of the scaled averages to a probability density tibble(x = seq(-4, 4, length.out = 10000)) %&gt;% mutate(y = dlogspline(x,logspline(all_exps$t))) %&gt;% ggplot(aes(x = x, y = y)) + geom_line(colour = &quot;darkblue&quot;) + labs(x = &quot;t&quot;, y = &quot;density&quot;) + xlim(c(-4,4)) + theme_minimal(12) -&gt; density_plot ((mean_hist | t_hist) / density_plot ) + plot_annotation(tag_levels = &quot;A&quot;) 1.2.1.2 Using our device We can use our statistical model of our device (our distribution of t values) to come up with a method for finding treasure. Our statistical model tells us what readings we’ll see when we haven’t found treasure and how often we’ll see those readings. In the absence of treasure we’ll see readings near the middle of the distribution very often and readings near the tails of the distribution less often. We might even say that, in the absence of treasure, it would be pretty surprising to see an extreme reading. Now we don’t know anything about how the device behaves when it’s around treasure, but we know what readings would be surprising if it wasn’t around treasure. We can use this fact to come up with a treasure hunting rule. When you see a surprising reading, dig for treasure. When you see an unsurprising reading, move on to the next spot. Let’s try it out! # 4 # trying out our treasure hunting device set.seed(151); X = rnorm(10, runif(1,-10,10), runif(1)); x_bar = mean(X) # work out an average s_x_bar = sd(X)/sqrt(length(X)) # work out the scaling factor t_value = x_bar / s_x_bar # work out the scaled measurement glue::glue(&quot;Our {length(X)} measurements are: {glue::glue_collapse(round(X,2),sep = &#39;; &#39;)} Our $\\\\bar{{x}}$ = {round(x_bar,3)} Our $s_\\\\bar{{x}}$ = {round(s_x_bar,3)} This means that our scaled measurement, $t$ = {round(t_value,3)}&quot;) Our 10 measurements are: 0.25; -0.54; -0.51; -0.75; 0.27; -1.62; -1.29; -0.21; 0.17; -0.83 Our \\(\\bar{x}\\) = -0.507 Our \\(s_\\bar{x}\\) = 0.204 This means that our scaled measurement, \\(t\\) = -2.485 Once we have our scaled reading, we can ask how surprising it is. To do this, we just compare it against the distribution of measurements that we generated when we weren’t around treasure. # 5 # how surprising is our measurement # first convert it to an absolute value t_value = abs(t_value) larger_than_positive = mean(all_exps$t &gt; t_value) smaller_than_negative = mean(all_exps$t &lt; -t_value) further_from_zero = larger_than_positive + smaller_than_negative closer_to_zero = 1 - further_from_zero glue::glue(&quot;{round(closer_to_zero * 100,2)}% of values from our simulation where closer to zero than our current value. Only {round(further_from_zero * 100, 2)}% of values where further from zero than our current value.&quot;) 96.1% of values from our simulation where closer to zero than our current value. Only 3.9% of values where further from zero than our current value. Once we have a measurement of how surprising our value is, then we just need to set a threshold for when it’s surprising enough to warrant digging. We’ll call this threshold \\(\\alpha\\), and we’ll set it to 5% (for literally no reason in particular). Now let’s try using the rule. We’ll do another simulation. We’ll simulate many many hunts, and on each hunt there either will be treasure or there won’t be treasure. Treasure will occur with the probability of \\(Pr_{\\mathrm{treasure}}\\). We won’t know this value, because we’ll just randomly set it. For each hunt, we’ll note done whether the rule told us to dig or move one. And we’ll also record the ground truth to test the accuracy. # 6 # testing our rule set.seed(14) # set seed for reproducibly if(inParallel){ n_tests = 10000 # set the number of tests } else { n_tests = 1000 } Pr_treasure = runif(1) # Set the probablity of finding treasure simulate_hunt &lt;- function(Pr_treasure){ # Decide whether this hunt has treasure has.treasure = ifelse(runif(1) &lt; Pr_treasure, 0, 1) X = rnorm(10, has.treasure, 1.5) # generate 10 readings # work out the scaled measurement and how suprising it is # and decide whether to dig or not! t.test(X) %&gt;% broom::tidy() %&gt;% mutate(has.treasure = ifelse(has.treasure == 1, &quot;Y&quot;,&quot;N&quot;), how.suprising = p.value, dig = ifelse(p.value &lt; 0.05, &quot;Y&quot;,&quot;N&quot;)) %&gt;% select(has.treasure, dig) } set.seed(11) # set seed for reproducibly test_hunts = map_df(1:n_tests, function(x) simulate_hunt(Pr_treasure)) # 7 # look at the results of tests test_hunts %&gt;% slice(1:20) %&gt;% knitr::kable(format = &quot;html&quot;, col.names = c(&quot;Had treasure?&quot;,&quot;Dig or not?&quot;)) Had treasure? Dig or not? Y Y Y N Y Y Y N Y Y N N Y N Y N Y N Y Y Y Y N N Y N Y N N N Y Y Y Y Y Y Y Y Y N To asses the usefulness of our rule, we can evaluate the accuracy of our rule. There are a few ways to do this. We can look at overall accuracy. We can look at how often we missed treasure when there was treasure. We can look how often dug for treasure when there wasn’t any. Let’s take a look at some metrics. #8 # function for computing metrics get_metrics = function(test_hunts){ test_hunts %&lt;&gt;% mutate(type = case_when(has.treasure == &quot;Y&quot; &amp; dig == &quot;Y&quot; ~ &quot;Hit&quot;, has.treasure == &quot;Y&quot; &amp; dig == &quot;N&quot; ~ &quot;Miss&quot;, has.treasure == &quot;N&quot; &amp; dig == &quot;N&quot; ~ &quot;Correct Rejection&quot;, has.treasure == &quot;N&quot; &amp; dig == &quot;Y&quot; ~ &quot;False alarm&quot;)) metrics = test_hunts %&gt;% mutate(type = factor(type,levels = c(&quot;Hit&quot;,&quot;Miss&quot;,&quot;Correct Rejection&quot;,&quot;False alarm&quot;))) %&gt;% group_by(type, `.drop` = FALSE) %&gt;% summarise(n = n()) %&gt;% mutate(type = as.character(type)) %&gt;% pivot_wider(names_from = &quot;type&quot;, values_from = &quot;n&quot;) %&gt;% mutate(Correct = `Correct Rejection` + Hit, Incorrect = `False alarm` + Miss, Accuracy = Correct / (Correct + Incorrect)) amount_of_treasure = test_hunts %&gt;% mutate(has.treasure = factor(has.treasure, levels = c(&quot;Y&quot;,&quot;N&quot;))) %&gt;% group_by(has.treasure, `.drop` = FALSE) %&gt;% summarise(n = n()) %&gt;% spread(key = has.treasure, value = n) %&gt;% set_colnames(c(&quot;N&quot; = &quot;No treasure&quot;, &quot;Y&quot; = &quot;Treasure&quot;)) %&gt;% mutate(`actual Pr_treasure` = `Treasure` / (`No treasure` + `Treasure`)) cbind(metrics,amount_of_treasure) } # 9 # Get some metrics for your simulated hunt test_hunts %&gt;% get_metrics() ## Hit Miss Correct Rejection False alarm Correct Incorrect Accuracy ## 1 3560 3947 2360 133 5920 4080 0.592 ## No treasure Treasure actual Pr_treasure ## 1 7507 2493 0.2493 The rule seems to work pretty well in terms of accuracy. But how much is accuracy dependent on the actual probability of finding treasure? Let’s run two more quick simulations where we set the probability of treasure actually being present to 1 or 0. # 10 test_hunts_nothing = map_df(1:n_tests, function(x) simulate_hunt(0)) test_hunts_nothing %&gt;% get_metrics() ## Hit Miss Correct Rejection False alarm Correct Incorrect Accuracy ## 1 4738 5262 0 0 4738 5262 0.4738 ## No treasure Treasure actual Pr_treasure ## 1 10000 0 0 # 11 test_hunts_everywhere = map_df(1:n_tests, function(x) simulate_hunt(1)) test_hunts_everywhere %&gt;% get_metrics() ## Hit Miss Correct Rejection False alarm Correct Incorrect Accuracy No treasure ## 1 0 0 9521 479 9521 479 0.9521 0 ## Treasure actual Pr_treasure ## 1 10000 1 But maybe just looking at accuracy isn’t the best. After all, there are two ways in which we can be wrong. We can dig when we’re not meant to, and we can move on when there’s actually treasure. So let’s split that accuracy percentage (or rather the [1 - accuracy] or “error” percentage) into two: 1) Digging when there’s no treasure, and 2) moving on without digging when there was treasure. Now let’s adjust \\(Pr_{\\mathrm{treasure}}\\) and see how the two error rates fare. # 12 get_metrics2 = function(.data){ .data %&gt;% mutate(`False alarm rate` = `False alarm`/(`No treasure` + `Treasure`), `Miss rate` = `Miss` / (`No treasure` + `Treasure`)) } test_hunts_nothing %&gt;% get_metrics() %&gt;% get_metrics2() ## Hit Miss Correct Rejection False alarm Correct Incorrect Accuracy ## 1 4738 5262 0 0 4738 5262 0.4738 ## No treasure Treasure actual Pr_treasure False alarm rate Miss rate ## 1 10000 0 0 0 0.5262 test_hunts_everywhere %&gt;% get_metrics() %&gt;% get_metrics2() ## Hit Miss Correct Rejection False alarm Correct Incorrect Accuracy No treasure ## 1 0 0 9521 479 9521 479 0.9521 0 ## Treasure actual Pr_treasure False alarm rate Miss rate ## 1 10000 1 0.0479 0 We can see that no matter what we do, the false positive rate (digging when there’s no treasure) never goes above 5%, which is the same value we set for \\(\\alpha\\). This is great because it means that we can with certainty set the upper bound of this error rate. And, we can do so knowing nothing about how much treasure there is to be found or how our device works in the presence of treasure. All we need is: 1) to know that on average the device points as zero when there’s no treasure around and 2) to sit and watch the device for a long time and just record some scaled measurements that the device produces. In fact, we don’t even need to do (2). We can just pretend to this by simulating the results, and we only need to input one parameter—that value we need for step 1. Everything else can just be made up. I’m not going to talk much about the other error rate, because this isn’t a course of frequentist inference. But we can estimate it based on some assumptions about how the device behaves in the presence of treasure. For example, if we know that treasure of a certain values results in the device pointing on average at 1, then we can calculate the upper bound of missing treasures smaller than that value. Limiting our error rates this way is pretty good, but we can do even better if we change the scenario a little. Let’s say that instead of hunting randomly for treasure, we’re sent shipments that contain collections of treasure boxes. For these collections, one of two things is true. Either all the boxes contain treasure or none of the boxes contain treasure. Now all we have to do, is run our device over the boxes and if the device produces “surprisingly large” deviations on more than \\((100 \\times \\alpha)\\%\\) of boxes we accept the whole lot. If our device indicated “surprisingly large” deviations on fewer than \\((100 \\times \\alpha)\\%\\) of boxes we send them all back. By doing this, we can guarantee that we’ll accept exactly 100% of shipments that have treasure and reject exactly 100% of shipments that don’t have treasure. And again, we get all this power from just running some pretend experiments (simulations). 1.3 Interim summary What this rather long-winded demonstration was meant to show is that p values are very good at doing one thing. That thing is, controlling how often we’re wrong in the long run. Deployed in the right context, we can even ensure that we’re never wrong. This all comes from a simple process: Setting the value of one parameter, running pretend experiments, and then comparing our data at hand to results obtained from our pretend experiments to judge whether our data is surprising or not. Of course, our treasure-hunting scenario, whether in the first form where we hunt randomly for treasure or in the second form where we examine batches that either do or don’t contain treasure, may not be exactly analogous to how science works. These means that deciding whether p values are useful or not is going to depend on how closely their real-world use case matches their ideal operating environment. 1.4 A short note on confidence intervals I’ll mention confidence intervals only briefly, but they follow the exact same logic as p-values. Let say I collect some measurements, work out the average. I could scale this value with my scaling factor and then turn to my list of results from the pretend experiments to work out my p value. However, I can also use the results from the pretend experiments to construct (the very poorly named) confidence interval. Looking at my pretend experiments I would see that any scaled values that are more than about 2.23 t units from 0 would be surprising. Using this information, I can ask myself the following question: If my device on average pointed at the current sample average, rather than zero, what data values would be surprising? The answer to this is, of course, values that are more than 2.23 t units from the sample mean. Having an answer in t units isn’t very useful. But I know that I converted measurements to t units by scaling readings using the scaling factor \\(s_{\\bar{x}}\\). This means we can just un-scale the value in t units back into raw units using the scaling factor calculated from my sample. This means I can say that any values \\(\\pm 2.23\\cdot{}s_{\\bar{x}}\\) from the sample mean would be surprising. Any values less than this, or in this range, would be unsurprising if my device, on average, pointed at my current sample mean. I could draw a line through these values, pull little tails on this line, and hey presto I have a confidence interval. Now just to be clear, just like a p value, a confidence interval tells you what values would be surprising/unsurprising on an assumption of a certain value of the parameter. For the p value you set the parameter to 0 (or wherever else the device points when no treasure is around). For the confidence interval, I just set the parameter to the value from my current sample. It’s exactly the same idea. Hopefully, this should make it clear what the confidence interval doesn’t tell you. It doesn’t tell you about the probability of a parameter falling within a range (the common misinterpretation). It tells you frequency with which data from pretend experiments will fall within a particular range on the assumption that the parameter is equal to the observed value. At no point are we making inferences about parameters or true values of parameters. We are holding parameters constant, doing pretend experiments, and then judging whether the data we have at hand is surprising or not in relation to those pretend experiments. Now that we’re all on the same page about p values and confidence intervals, we’re aware of their power, and where they get this power from, let’s look at some criticisms of p values. "],
["criticisms-of-p-values.html", "Chapter 2 Criticisms of p values 2.1 Same measurements from different devices 2.2 The universe of possible events", " Chapter 2 Criticisms of p values People have written lots of criticism of p-values. A lot of these are of the form “p-values are bad because they don’t do X”, where X is not a design feature of frequentist inference. I’m not interested in these kinds of criticisms, because they seem pretty meaningless. Instead, I think that we are going to criticise p-values it is better to look at the design features of frequentist inference and find fault there. So what are the design features? In the last section, we saw how frequentist inference was very good at controlling the kinds of mistakes we made in our treasure hunt. To do this, all we needed was a model of how our treasure detecting device operated. If we only wanted to control false positives all we needed was a model of how it operated in the absence of treasure. To build this model, we just need one bit of information—that the dial on average pointed at 0 when there was no treasure. The whole model could then be built up by running lots of simulations (or pretend experiments) where this parameter (the average reading in the absence of treasure) was the only parameter we needed to set. That’s a pretty powerful property. And we get it all from running imaginary experiments! 2.1 Same measurements from different devices Now let’s imagine a new scenario. As before, you have a treasure hunting device (we’ll call it \\(D_1\\)). You’re using \\(D_1\\) to hunt treasure, using the readings to decide whether to dig or not. At your first treasure hunting spot, you record the measurements: 1, 0, 1, 3, 0, 1, 4, -1, 3, 4. You then average, and scale these measurements and get a t value of approximately 2.848. You compare this to what you found in your imaginary experiments and find p = .019. According to your rule, that means you dig. But before you start digging, I run up to you and tell you that device \\(D_1\\) is broken. I tested it before you left, and found that \\(D_1\\) is incapable of measuring values bigger than 6. You look at your measurements again, and to your relief, they don’t go anywhere near 6. Your highest measurement is only 4. But should you worry that it couldn’t register values of 6 or higher? And if so, why? More generally, how would this fault influence your treasure hunting strategy and would it change your view on when you think you should start digging? The intuition here might be a little unclear, so let’s modify the example a little bit. In the modified example, you have two measurement devices (\\(D_1\\) and \\(D_2\\)). You’re told they’re identical and, indeed, when you look at the measurements you can see that they’ve recorded an identical set of 10 numbers. Because the measurements are the same, you just pick whichever device to work out your scaled reading and decide whether to dig. But now I again tell you that \\(D_1\\) is actually broken and it is incapable of recording measurements higher than 6. I also tell you \\(D_2\\) is working just fine. What does this do to your inference? Does your inference change depending on whether you decided to look at \\(D_1\\) or \\(D_2\\)? If you want to be a good frequentist then the answer to this question is a resounding yes. Even though \\(D_1\\) and \\(D_2\\) produced the exact same measurements, and despite these measurements being accurate, your inference will depend on the device you decided to look at. But why? Understanding the answer to this means going back to our pretend experiments. Let’s run some pretend experiments for \\(D_1\\) and \\(D_2\\). The stimulations for \\(D_1\\) will be modified slightly so that all values higher than 6 will be replaced with a 6. # 1 # code for simulated experiments for D1 run_exp_D1 = function(){ n = 10 X = runif(n, -20, 20) # replace all values values greater than 6 with 6 X = map_dbl(X, function(x) ifelse(x &gt; 6, 6, x)) m = mean(X) s = sd(X) t = m/(s/sqrt(n)) } # code for simulated experiments for D2 run_exp_D2 = function(){ n = 10 X = runif(n, -20, 20) m = mean(X) s = sd(X) t = m/(s/sqrt(n)) } # 2 # run the simulations inParallel = TRUE # set to TRUE for local or FALSE for cloud if(inParallel){ future::plan(multiprocess) no_of_exps = 100000 # Set the number of experiments to simulate map_df &lt;- furrr::future_map_dfr map_dbl &lt;- furrr::future_map_dbl } else { no_of_exps = 1000 } D1_exps = map_dbl(1:no_of_exps, function(x) run_exp_D1()) D2_exps = map_dbl(1:no_of_exps, function(x) run_exp_D2()) # 3 # draw distributions for D1 and D2 tibble(D1 = D1_exps, D2 = D2_exps) %&gt;% pivot_longer(cols = c(&quot;D1&quot;,&quot;D2&quot;), values_to = &quot;x&quot;) %&gt;% ggplot(aes(x = x,fill = name)) + geom_histogram(alpha = .5, bins = 50, na.rm = TRUE) + xlim(c(-4,4)) + scale_fill_manual(values = c(&quot;D1&quot; = &quot;darkred&quot;,&quot;D2&quot; = &quot;darkblue&quot;), labels = c(&quot;D1&quot; = &quot;Device D₁&quot;, &quot;D2&quot; = &quot;Device D₂&quot;), name = NULL) + theme_minimal(12) + theme(legend.position = &quot;top&quot;) + labs(y = &quot;number of experiments&quot;, x = &quot;t&quot;) As you can see, the distributions are different. This is because in those pretend experiments, the devices would behave differently. In our actual experiment (this treasure hunt), they didn’t behave differently. They behaved exactly the same, and both behaved accurately. However, there’s no getting away from the fact that because the devices have the potential to behave differently in situations other than the current situation, this potential difference must be accounted for. They factor into the calculation of the p value by changing the distributions and, therefore, we need to take account of these potential events in our inferences if we want to maintain our error control properties. For some, the influence of imaginary events is madness. Jeffreys described this “madness” as follows: What the use of P implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred. This seems a remarkable procedure (Jeffreys, 1961, p. 385) 2.2 The universe of possible events To see another example of how potential events can influence inference, let’s examine a different scenario. In this scenario, we’re going to make judgements about the fairness of a coin (fair coins being defined as coins that show heads with \\(Pr_{\\mathrm{heads}}\\) = 0.5). We’ll use the same procedure as our treasure hunting device. We will flip a coin that we know is fair a set number of times. We will 10 times again, but for no particular reason. We then just count up \\(x\\) heads out of our total of \\(n\\) flips. We then repeat the procedure many many times. We can use this procedure to generate a distribution of possible data. Again, we can just simulate this. # 4 # define function for flipping coins do_flips = function(n_flips = 10, pr_heads = .5){ # set default to 10 flips with a fair coin # generate n_flips bernoulli trials and count number of heads tibble(n = n_flips) %&gt;% mutate(h = sum(purrr::rbernoulli(n,pr_heads)),t = n - h) } # 5 # run this chunk to actually run the simulations! n_flips = 10; pr_heads = .5 all_flips = map_dfr(1:no_of_exps, function(x) do_flips(n_flips, pr_heads)) # 6 # plot the distribution of results from the simulated experiments all_flips_summary = all_flips %&gt;% add_count(name = &quot;total&quot;) %&gt;% group_by(h,total) %&gt;% summarise(d_heads = n()) %&gt;% mutate(d_heads = d_heads/total) %&gt;% ungroup() ggplot(all_flips_summary, aes(x = h, y = d_heads)) + geom_point(size = 4, colour = &quot;darkblue&quot;) + geom_line() + theme_minimal(12) + scale_x_continuous(name = &quot;number of heads&quot;, limits = c(0,10), breaks = seq(0,10,2)) + scale_y_continuous(name = &quot;relative frequency&quot;) Armed with this distribution, we can start making judgements about actual data. To produce some real data, I flip a coin, and at the end, I count up 8 heads and 2 tails. Now you can make a judgement about whether this data is surprising or not. To do this, all you need to do is compare it to the simulated results above. # 7 # compare current results to simulated results to see whether it is surprising x = 8 # the number of heads in our current sample x_heads_if_fair = n_flips * pr_heads # the average number of heads for a fair coin distance_of_sample_from_fair = x - x_heads_if_fair # how far is this sample from the fair mean coin_flip_p = all_flips_summary %&gt;% mutate(deviation = abs(mean(h) - h)) %&gt;% filter(deviation &gt;= distance_of_sample_from_fair) %&gt;% pull(d_heads) %&gt;% sum() glue::glue(&quot;The *p* value for {x} heads in {n_flips} flips is {round(coin_flip_p,3)} This result is {if_else(coin_flip_p &gt; 0.05, &#39;not surprising&#39;,&#39;surprising&#39;)} on the assumption that the coin is fair (i.e., $Pr_{{\\\\mathrm{{heads}}}}$ = {pr_heads})&quot;) The p value for 8 heads in 10 flips is 0.107 This result is not surprising on the assumption that the coin is fair (i.e., \\(Pr_{\\mathrm{heads}}\\) = 0.5) But save your judgement for now, because there’s something that I have neglected to tell you. My plan wasn’t to flip the coin 10 times. Instead, I decided that I would just flip the coin until it came up tails twice, and it just so happened that on this occasion, this meant that I flipped the coin 10 times. Does this fact change your inference? If our inferences are based on comparing our actual data to possible data, we need to take into account how deciding to flip the coin until it came up tails twice changes the possible data that could’ve been produced by this experiment. For example, on a different occasion, getting 2 tails might have only required 2 flips, or 9 flips, or 15 flips. Is it surprising that it (only) took 10 flips this time? How often would we have to flip the coin 10 or more times before we got 2 tails? To answer this we’ll need to re-run the simulation, and this time only stop after we get 2 heads. # 8 # define function for new simulation. stop after stop_at heads do_flips_until = function(stop_at = 2, pr_heads = 0.5){ tails = 0 flips = 0 while(tails &lt;= (stop_at - 1)){ tails = tails + purrr::rbernoulli(1,pr_heads); flips = flips + 1} return(flips) } # 9 # run the actual simulations stop_at = 2; pr_heads = .5 all_flips_v2 = map_dbl(1:no_of_exps, function(x) do_flips_until(stop_at, pr_heads)) We now can count up the relative frequency of getting 2 heads after 2 flips, after 3 flips, 4 flips, and so on. And we can draw a plot of this distribution. # 10 # plot the distribution of results from the simulation experiments # make a frequency table all_flips_v2_freq = summarise(group_by(enframe(all_flips_v2),value), n = n()) %&gt;% set_colnames(c(&quot;flips&quot;,&quot;counts&quot;)) %&gt;% mutate(freq = counts / sum(counts), flips = as.numeric(flips), counts = as.numeric(counts), freq = as.numeric(freq)) # make the actual plot all_flips_v2_freq %&gt;% ggplot(aes(x = flips, y = freq)) + geom_point(size = 4, colour = &quot;darkblue&quot;, na.rm = TRUE) + geom_line(na.rm = TRUE) + theme_minimal(12) + scale_x_continuous(name = &quot;number of flips before stopping&quot;, limits = c(2,16), breaks = seq(2,16,2)) + scale_y_continuous(name = &quot;relative frequency&quot;) From this new distribution, we can now ask: How often would you need to flip a fair coin 10 or more times before you got two heads? That is, is it surprising that we had to flip it this many times? Let’s see how the inference differs. # 11 # calculate a p value based on our new simulations observed_flips = 10 # how many flips did we observe coin_flip_v2_p = all_flips_v2_freq %&gt;% filter(flips &gt;= observed_flips) %&gt;% pull(freq) %&gt;% sum() glue::glue(&quot;For a fair coin ($Pr_{{\\\\mathrm{{heads}}}}$ = {pr_heads}), about {round(100 - coin_flip_v2_p * 100,2)}% of experiments would end before we got to {observed_flips} flips. Only {round(coin_flip_v2_p * 100,2)}% of experiments would run this long. Therefore, our result is {if_else(coin_flip_v2_p &lt; 0.05, &#39;surprising!&#39;,&#39;not surprising.&#39;)}&quot;) For a fair coin (\\(Pr_{\\mathrm{heads}}\\) = 0.5), about 98.04% of experiments would end before we got to 10 flips. Only 1.96% of experiments would run this long. Therefore, our result is surprising! What these two examples show is that even when presented with the same data the inferences will be different if the realm of possible, but not actual results are different. That is, non-existent results influence our inferences. Based on this fact, we can go ahead to imagine even more ridiculous examples. For example, imagine that I build a device that is going to decide whether 1) to flip the coin n times or 2) flip it until it comes up tails x times. The device makes a decision, flips the coin, and it just so happens on this occasion to show 8 heads and 2 tails. How do I analyse this set of data? Does the realm of possible data include the machine that makes the decision? What if I know what decision the device made? Do I still have to take into account the experiment that wasn’t performed? And what if I have the results of two experiments, one that was performed as part of a mixture (using a machine to decide which of the two experiments would be performed) and one which was performed not as part of a mixture. If they yield the same data, then does the fact that one was part of a mixture mean that the conclusions should be different? For a frequentist, these can be pretty uncomfortable questions! "],
["the-evidential-alternative-to-p-values.html", "Chapter 3 The evidential alternative to p values 3.1 A theory of statistical evidence", " Chapter 3 The evidential alternative to p values Coming up with an alternative to p values requires us to rearrange our thinking a bit. So let’s first again get straight what we’re doing with frequentist inference. In frequentist inference we set some parameter to a certain value (\\(\\theta\\)), we then generate data from imaginary experiments using that parameter setting, and we then compare our data to the data from those experiments and ask: \"how consistent is our data with the data that would be generated if the parameter was actually \\(\\theta\\)? At no point are we making inferences about the value of \\(\\theta\\). We set the value. To think about what an alternative might look like, let us think back to our earlier example on the different meanings of probability. With p-values we thought about probability in terms of relative frequency. We were asking “how often?” questions. But I also mentioned another example. The example of being 90% sure that the accused committed a crime. If we want to be rational humans, when we make claims like this, what we usually do is examine the evidence. We compare whether there is more evidence for the accused’s guilt or the accused’ innocence. That is, we take the courtroom evidence and examine whether it supports hypothesis 1 (the accused is guilty) or hypothesis 2 (the accused is innocent). But what is statistical evidence? 3.1 A theory of statistical evidence To understand the concept of statistical evidence, let’s go back to our coin flipping example. In our coin flipping example, we collected 10 flips and found 8 heads and 2 tails. Our frequentist analysis asked something like, \" how consistent is this data with the data that would’ve been generated if the bias was 0.5?\" But we could ask another question. That question might go something like this: “Given this data, is it more likely that the bias is 0.6 or that the bias is 0.8?” To answer this question, we’ll set our parameter \\(\\theta\\) to the values between 0 and 1. For each setting, we’ll simulate a bunch of experiments, and then we’ll ask, “how often did our exact result (8 heads in 10 flips) occur with that setting of \\(\\theta\\)?” We’ll also do it twice. In one version, we will flip the coin 10 times. In the other version, we’ll carry on flipping until we get two tails. I’m also going to cheat slightly. I’m not actually going to simulate the results. I know the distribution they’ll follow, so I’ll just compute the distributions directly. #1 # set up the functions that plot the distributions of results for # various values of pr_heads # function v1 &quot;simulates&quot; results from the version where you # flip the coin n times and then count up the heads. # three parameters need to be set # 1. the number of flips (n_flips) # 2. the probability of heads (pr_heads) # 3. the number of heads in our observation (obs_heads) coin_flip_v1 = function(n_flips, pr_heads, obs_heads){ pmap_df(tibble(heads = 1:n_flips, flips = n_flips, pr_heads = pr_heads), # input values function(heads, flips, pr_heads) tibble(flips = flips, heads = heads, freq = dbinom(heads, flips, pr_heads))) %&gt;% # get the frequency mutate(our_ob = case_when(flips == n_flips &amp; heads == obs_heads ~ TRUE, TRUE ~ FALSE)) # mark our observation } # function v2 &quot;simulates&quot; results from the version where you # flip the coin until it comes up tails n times. # three parameters need to be set # 1. the number of tails to stop at (n_tails) # 2. the probability of heads (pr_heads) # 3. the number of flips in our observation (obs_flips) coin_flip_v2 = function(n_tails,pr_heads,obs_flips){ pmap_df(tibble(tails = n_tails, pr_heads = pr_heads, flips = 1:(obs_flips+4)), # input values function(tails, pr_heads, flips) tibble(flips = flips, tails = tails, freq = dnbinom(flips - tails, tails, 1 - pr_heads))) %&gt;% mutate(our_ob = case_when(flips == obs_flips &amp; tails == tails ~ TRUE, TRUE ~ FALSE)) # mark our observation } Once we’ve set up our functions, we can draw the distributions of the possible data that would occur for different values of \\(Pr_{\\mathrm{heads}} = \\theta\\). I’ll just pick a few. In each of the plots, our actual observation will be highlighted. Although we’re “simulating” all possible observations, we only care about our actual observation. And we want to know the relative frequency with which that result occurs, not the frequency of results that didn’t but might’ve occurred. # 2 # draw distributions of the data for various values of pr_heads for version 1 (flip n times) n_flips = 10 pr_heads_values = c(1/10,5/10,9/10) # set our pr_heads values obs_heads = 8 # make the plots coin_flip_v1_plots = pmap(tibble(n_flips = n_flips, pr_heads = pr_heads_values, obs_heads = obs_heads), function(n_flips, pr_heads, obs_heads) coin_flip_v1(n_flips, pr_heads, obs_heads) %&gt;% ggplot(aes(x = heads, y = freq)) + geom_line(alpha = .25) + geom_point(aes(colour = our_ob), size = 3) + scale_colour_manual(guide = &quot;none&quot;, values = c(&quot;TRUE&quot; = &quot;black&quot;, &quot;FALSE&quot; = &quot;grey&quot;)) + labs(x = glue(&quot;number of heads in {n_flips} flips&quot;), y = &quot;relative frequency&quot;, title = glue(&quot;Pr heads = {pr_heads}&quot;)) + theme_minimal()) # make the plots pretty and arrange them coin_flip_v1_plots = map(coin_flip_v1_plots, function(x) x + scale_x_continuous(breaks = seq(0,10,2)) + scale_y_continuous(limits = c(0,.5)) + theme(axis.text = element_text(size = 10), axis.title = element_text(size = 10))) patchwork::wrap_plots(coin_flip_v1_plots,nrow = 3) + plot_annotation(tag_levels = &quot;A&quot;) # 3 # draw distributions of the data for various values of pr_heads for version 2 (flip until n tails) n_tails = 2 pr_heads_values = c(1/10,5/10,9/10) # set our pr_heads values obs_flips = 10 # make the plots coin_flip_v2_plots = pmap(tibble(n_tails = n_tails, pr_heads = pr_heads_values, obs_flips = obs_flips), function(n_tails,pr_heads,obs_flips) coin_flip_v2(n_tails,pr_heads,obs_flips) %&gt;% ggplot(aes(x = flips, y = freq)) + geom_line(alpha = .25, na.rm = TRUE) + geom_point(aes(colour = our_ob), size = 3, na.rm = TRUE) + scale_colour_manual(guide = &quot;none&quot;, values = c(&quot;TRUE&quot; = &quot;black&quot;, &quot;FALSE&quot; = &quot;grey&quot;)) + labs(x = glue(&quot;number of flips for {n_tails} tails&quot;), y = &quot;relative frequency&quot;, title = glue(&quot;Pr heads = {pr_heads}&quot;)) + theme_minimal()) # make the plots pretty and arrange them coin_flip_v2_plots = map(coin_flip_v2_plots, function(x) x + scale_x_continuous(breaks = seq(2,12,2), limits = c(2,12)) + scale_y_continuous(limits = c(0,1)) + theme(axis.text = element_text(size = 10), axis.title = element_text(size = 10))) patchwork::wrap_plots(coin_flip_v2_plots,nrow = 3) + plot_annotation(tag_levels = &quot;A&quot;) Since we just want to know the relative frequency of our specific observation so we’ll draw two more plots that just show the relative frequency of our observation for each value of \\(Pr_{\\mathrm{heads}}\\). That is, we’re going to take all the highlighted points from the plots above and put them on a single plot. We’ll generate a plot for each version. # 4 # first set our observation again number_of_heads = 8 number_of_flips = 10 # and set the range of bias to consider pr_heads_range = seq(0,1,.1) # 0/10, 1/10 ... 9/10, 10/10 # translate our observation into the parameters needed for version 1 # generate the data and pull out the relative frequency of our specific observation n_flips = number_of_flips obs_heads = number_of_heads likelihood_v1 = map_df(pr_heads_range, function(x) coin_flip_v1(n_flips, x, obs_heads) %&gt;% filter(our_ob == TRUE) %&gt;% select(freq) %&gt;% mutate(pr_heads = x)) # translate our observation into the parameters needed for version 2 # generate the data and out the relative frequency of our specific observation n_tails = number_of_flips - number_of_heads obs_flips = number_of_flips likelihood_v2 = map_df(pr_heads_range, function(x) suppressWarnings( # supress warnings about impossible values coin_flip_v2(n_tails,x,obs_flips) %&gt;% filter(our_ob == TRUE) %&gt;% select(freq) %&gt;% mutate(pr_heads = x) %&gt;% mutate(freq = ifelse(is.na(freq), 0, freq)))) # replace NaN (impossible values) with 0 for plotting # 5 # now actually draw the plots likelihood_v1_plot = likelihood_v1 %&gt;% ggplot(aes(x = pr_heads, y = freq)) + geom_point() + geom_line() + theme_minimal(12) + scale_x_continuous(name = &quot;Pr heads&quot;, breaks = seq(0,1,.2)) + scale_y_continuous(limits = c(0,.4), name = &quot;relative frequency&quot;) + labs(title = glue(&quot;likelihood function for {number_of_heads} heads in {number_of_flips} flips&quot;), subtitle = glue(&quot;stopping after {number_of_flips}&quot;)) likelihood_v2_plot = likelihood_v2 %&gt;% ggplot(aes(x = pr_heads, y = freq)) + geom_point() + geom_line() + theme_minimal(12) + scale_x_continuous(name = &quot;Pr heads&quot;, breaks = seq(0,1,.2)) + scale_y_continuous(limits = c(0,.4), name = &quot;relative frequency&quot;) + labs(tile = glue(&quot;likelihood function for {number_of_heads} in {number_of_flips}&quot;), subtitle = glue(&quot;stopping after {n_tails} tails&quot;)) likelihood_v1_plot / likelihood_v2_plot + plot_annotation(tag_levels = &quot;A&quot;) The first set of plots looked very different. This difference is mainly in terms of the relative frequency of results that we didn’t observe. The relative frequency of the results we did observe is actually very similar. In the plots above they may not look that similar, but they are in fact just scaled versions of each other. We can scale them so that they match. In the scaled versions below, each plot is just scaled so that the peak is at 1. # 61 # now actually draw the scaled plots likelihood_v1_plot = likelihood_v1 %&gt;% mutate(freq = freq / max(freq)) %&gt;% ggplot(aes(x = pr_heads, y = freq)) + geom_point() + geom_line() + theme_minimal(12) + scale_x_continuous(name = &quot;Pr heads&quot;, breaks = seq(0,1,.2)) + scale_y_continuous(limits = c(0,1), name = &quot;relative frequency&quot;) + labs(title = glue(&quot;likelihood function for {number_of_heads} heads in {number_of_flips} flips&quot;), subtitle = glue(&quot;stopping after {number_of_flips}&quot;)) likelihood_v2_plot = likelihood_v2 %&gt;% mutate(freq = freq / max(freq)) %&gt;% ggplot(aes(x = pr_heads, y = freq)) + geom_point() + geom_line() + theme_minimal(12) + scale_x_continuous(name = &quot;Pr heads&quot;, breaks = seq(0,1,.2)) + scale_y_continuous(limits = c(0,1), name = &quot;relative frequency&quot;) + labs(tile = glue(&quot;likelihood function for {number_of_heads} in {number_of_flips}&quot;), subtitle = glue(&quot;stopping after {n_tails} tails&quot;)) likelihood_v1_plot / likelihood_v2_plot + plot_annotation(tag_levels = &quot;A&quot;) The plots describe the relative frequency of observing our result given different values of the coin bias. They both show that our observation (8 heads in 10 flips) would be the most frequent observation if the coin was in fact biased to show 8 heads in 10 flips. We can call these plots likelihood functions and we can call the value of the bias that corresponds to the peak of the likelihood function the maximum likelihood estimate. Before we go further, it is worthwhile to pay attention to the key difference between these plots and the plots we’ve drawn up until now. In all our previous plots we plotted the data you would expect for a given value of a parameter (e.g., the coin bias, or where the treasuring hunting device arrow points when not around treasure). These plots show something very different. They show relative frequency of our observation for different parameter values. Put another way: In the first set of plots, we fix our parameter value while the data varies. In the second set of plots, we fix our data while the parameter varies. Or put yet another way: The first set of plots allows us to make inferences about data—is our data surprising on the assumption that the parameter has a specific value. The second set of plots will allow us to make inferences about parameters—what is the most likely value of the parameter given our specific data. 3.1.1 Some features of likelihoods Before we move to talk about how to use likelihoods in a notion of statistical evidence, it’s worth noting a couple of things about likelihoods. First, likelihoods are not probability distributions. This much should already be obvious by looking at the two unscaled likelihoods above. Probability distributions have an area under the curve of 1. That is, probability distributions describe the probability of events where the probability of every possible event is given (and where at least one of the events must occur). Summing these values gives the probability of an event (any event) occurring. This value is 1. And in fact, it’s given by our definition because our definition said that one of the events must occur. Our two likelihood functions do not necessarily have an area underneath equal to 1. For our two likelihoods from the coin flipping example, this is clear. Both can’t have an area equal to 1 because they both obviously have different areas. Instead, what the likelihood describes is the probability of a specific event occurring over the range of possible parameter values. Of course, probability and likelihood are related concepts. One is just in the inverse of the other, which is why likelihood is sometimes called “inverse probability”. Below we see some examples of how the area under the curve can vary. # 7 # plot some likelihood functions list(p1 = c(heads = 0, flips = 10),p2 = c(heads = 1, flips = 2),p3 = c(heads = 0, flips = 2),p4 = c(heads = 2, flips = 5)) -&gt; scenarios map(scenarios, function(x) {dbinom.like &lt;- function(t) dbinom(x[[&quot;heads&quot;]],x[[&quot;flips&quot;]],t); integrate(dbinom.like,0,1)$value %&gt;% tibble(heads = x[[&quot;heads&quot;]], flips = x[[&quot;flips&quot;]], auc = .)}) -&gt; aucs map(aucs, function(x) ggplot(mapping = aes(x = seq(0,1,length.out = 100), y = dbinom(x$heads,x$flips,seq(0,1,length.out = 100)))) + geom_line() + labs(y = &quot;likelihood&quot;, x = &quot;θ&quot;, title = glue::glue(&quot;likelihood for {x$heads} heads in {x$flips} flips&quot;), subtitle = glue::glue(&quot;area under curve is {round(x$auc,3)}&quot;)) + theme_minimal(12) ) -&gt; plots (plots$p1 + plots$p2) / (plots$p3 + plots$p4) It is also worth noting that in addition to having a peak at a certain point, likelihood functions also have a spread. We’ll be talking about this spread, or variance, at a later point. 3.1.2 Likelihoods and statistical evidence How do likelihood functions give us a notion of statistical evidence? One way to understand statistical evidence is to compare how likely our data is when \\(Pr_{\\mathrm{heads}}\\) = 0.6 and when \\(Pr_{\\mathrm{heads}}\\) = 0.8 by subtracting the two likelihood values. But because likelihoods can be on different scales, raw distances can be misleading. Instead, let us take the ratio. That way, we can tell how many times bigger one value is relative to the other. That is, are the two values the same size? Is one twice as big, three times bigger, etc. By taking the ratio we can now answer the question, “how many times more likely is our observation when \\(Pr_{\\mathrm{heads}}\\) = 0.6 relative to when \\(Pr_{\\mathrm{heads}}\\) = 0.8?” We can read this as the support our data gives to these two hypotheses. Let’s do this our observation and the likelihoods for the two versions of the coin-flipping example. Again, I won’t rely on simulations (although I could). Instead, I know that the two likelihoods are drawn from a binomial and a negative binomial distribution, respectively, so I’ll just generate the values. # 8 # set out observation n = 10 # number of flips h = 8 # number of heads t = n - h # number of tails # set our two observation h1 = 0.6 # the probability of getting heads is 0.6 h2 = 0.8 # the probability of getting heads is 0.8 L1 = dbinom(h,n, h1) L2 = dbinom(h,n, h2) glue::glue(&quot;The likelihood under $\\\\mathcal{{H}}_1$ ($Pr_{{heads}}$ = {h1}) is {round(L1,2)} The likelihood under $\\\\mathcal{{H}}_2$ ($Pr_{{heads}}$ = {h2}) is {round(L2,2)} The likelihood ratio is {round(L1/L2,2)} The data are {round(L1/L2,2)} times more probable under $\\\\mathcal{{H}}_1$ than $\\\\mathcal{{H}}_2$&quot;) The likelihood under \\(\\mathcal{H}_1\\) (\\(Pr_{heads}\\) = 0.6) is 0.12 The likelihood under \\(\\mathcal{H}_2\\) (\\(Pr_{heads}\\) = 0.8) is 0.3 The likelihood ratio is 0.4 The data are 0.4 times more probable under \\(\\mathcal{H}_1\\) than \\(\\mathcal{H}_2\\) # 9 # Set our observation n = 10 # number of flips h = 8 # number of heads t = n - h # number of tails # set our two hypotheses h1 = 0.6 # the probability of getting heads is 0.6 h2 = 0.8 # the probability of getting heads is 0.8 L1 = dnbinom(t,h,h1) L2 = dnbinom(t,h,h2) glue::glue(&quot;The likelihood under $\\\\mathcal{{H}}_1$ ($Pr_{{heads}}$ = {h1}) is {round(L1,2)} The likelihood under $\\\\mathcal{{H}}_2$ ($Pr_{{heads}}$ = {h2}) is {round(L2,2)} The likelihood ratio is {round(L1/L2,2)} The data are {round(L1/L2,2)} times more probable under $\\\\mathcal{{H}}_1$ than $\\\\mathcal{{H}}_2$&quot;) The likelihood under \\(\\mathcal{H}_1\\) (\\(Pr_{heads}\\) = 0.6) is 0.1 The likelihood under \\(\\mathcal{H}_2\\) (\\(Pr_{heads}\\) = 0.8) is 0.24 The likelihood ratio is 0.4 The data are 0.4 times more probable under \\(\\mathcal{H}_1\\) than \\(\\mathcal{H}_2\\) One thing you’ll notice is that regardless of the experiment type (flipping 10 times or flipping until 2 tails) the support for one hypothesis over the other is the same. This shouldn’t be surprising, because what lead to different inferences under the frequentist analysis was data that could’ve occurred but didn’t. Since we’re now only interested in the actual event that happened, these possible, but not actual, events don’t come in to play. So comparing likelihoods (by taking their ratio) can tell us which hypothesis is better supported by the data. However, there’s a couple of problems with this. First, how do we know when we have enough evidence? To answer this question, we’re going to have to take into account a lot of additional factors. And the answer to this question is probably going to be context-dependent. For example, if we’re placing bets on hypotheses, we’re probably going to want to take into account the relative pay-offs. If we’re using evidence to decide somebody’s guilt in a court case, we’re probably going to want to take into account things like “reasonable doubt”. In short, there’s not a straight forward answer to this question, so we’ll set it aside for now. Instead, we’ll turn to the second problem. The second problem with this method is that it only works for comparing two hypotheses. We can can say, for example, whether the data supports \\(Pr_{\\mathrm{heads}}\\) = 0.5 over the hypothesis \\(Pr_{\\mathrm{heads}}\\) = 0.8, and we can quantify this level of support. But usually, we are not comparing two simple hypotheses like this. Our hypotheses take a more complex form like: “Is the coin fair?” How might we go about answering this question? Let’s formalise this idea a bit. First, we’ll drop the terminology hypothesis, and replace it with model (denoted \\(\\mathcal{M}_H\\). And we’ll say that our data favours \\(\\mathcal{M_1}\\) over \\(\\mathcal{M_2}\\) by a factor of \\(\\frac{\\mathcal{M}_1}{\\mathcal{M}_2}\\). And second, we’ll say that the value of \\(\\mathcal{M}_H\\) is going to be the average likelihood of all the \\(n\\) sub-hypotheses (\\(\\Theta=\\left \\{ \\theta_1, \\theta_2,\\ldots\\theta_n\\right \\}\\)) that make up \\(\\mathcal{H}_M\\). Or, \\[\\mathcal{M}_H = \\sum_{i=1}^{n}\\mathcal{L}_H(\\theta_i|\\mathbf{y})\\cdot{}\\frac{1}{n}\\] From this, we can build a model of a fair coin, which might only have one sub-hypothesis (i.e., \\(\\theta\\) = 0.5). And we might want to compare that to a model that says the coin can can have a bias that takes any values from \\(\\frac{0}{10}\\) to \\(\\frac{10}{10}\\) (i.e., \\(\\theta = \\{\\frac{0}{10},\\frac{1}{10},\\frac{2}{10},\\frac{3}{10},\\frac{4}{10},\\frac{5}{10},\\frac{6}{10},\\frac{7}{10},\\frac{8}{10},\\frac{9}{10},\\frac{10}{10}\\}\\)). Let’s try to do that. We don’t need to do it for both versions any more. We can just do it for one of the versions because now we know that they’re same! # 10 # Set our observation n = 10 # number of flips h = 8 # number of heads # set our two hypotheses h1 = 0.5 # the probability of getting heads is 0.5 h2 = seq(0,1,.1) # the probability of getting heads is all possible values from 0/10 to 10/10 in steps of 1/10 M0 = dbinom(h,n, h1) M1 = dbinom(h,n, h2); M1 = mean(M1) glue::glue(&quot;The likelihood under $\\\\mathcal{{M}}_0$ is {round(M0,2)} The likelihood under $\\\\mathcal{{M}}_1$ is {round(M1,2)} The likelihood ratio is {round(M0/M1,2)} The data are {round(M0/M1,2)} times more probable under $\\\\mathcal{{M}}_0$ than $\\\\mathcal{{M}}_1$ The data are {round(M1/M0,2)} times more probable under $\\\\mathcal{{M}}_1$ than $\\\\mathcal{{M}}_0$&quot;) The likelihood under \\(\\mathcal{M}_0\\) is 0.04 The likelihood under \\(\\mathcal{M}_1\\) is 0.08 The likelihood ratio is 0.53 The data are 0.53 times more probable under \\(\\mathcal{M}_0\\) than \\(\\mathcal{M}_1\\) The data are 1.87 times more probable under \\(\\mathcal{M}_1\\) than \\(\\mathcal{M}_0\\) 3.1.3 Are all sub-hypotheses equal? Now that we have a way to compare two models that might be composed of one or more sub-hypotheses, we might reasonably ask, “are all sub-hypotheses equal?”. That is, do we want to just take a regular average over all the sub-hypotheses or might we instead want to take a weighted average? One good reason for taking a weighted average is that it allows us to take into account our views about what it means for a coin to be biassed or unfair. For example, we might think that if coins are biassed, then they’re much more likely to show heads. Or we might think that if a coin is biased, it is much less likely to show heads. Or maybe biased coins are only slightly more likely to show a preponderance of heads or tails. We can capture this view by weighting some sub-hypothesis more than others before taking our average. In fact, in our first calculation, we did apply a weighting, but it was hidden. But we can make it explicit. For \\(\\mathcal{M}_0\\) we can give the sub-hypothesis \\(\\theta\\) = 0.5 a weight of 1, and all others possible sub-hypotheses a weight of 0. For \\(\\mathcal{M}_1\\), we can give each of the \\(n\\) sub-hypotheses a weight of \\(\\frac{1}{n}\\). Ideally, we want all the weights to sum to 1. This way, we can think of the weights as a probability distribution. Usually these weights, or the weighting distribution, is refereed to as a prior (or occasionally as the model of the hypothesis). We can make these weights explicit by adding them in to our equation for \\(\\mathcal{M}_H\\). Our equation now becomes: \\[\\mathcal{M}_H = \\sum_{i=1}^{n}\\mathcal{L}_H(\\theta_i|\\mathbf{y})\\cdot{}p(\\theta_i)\\] Let’s explicitly adds the weights that were only implicit before. We’ll see that adding these weights scales down both our likelihood values, but it produces the ratio and hence the same result. # 11 # Set our observation n = 10 # number of flips h = 8 # number of heads # set our two hypotheses h1 = seq(0,1,.1) # the probability of getting heads is all possible values from 0/10 to 10/10 in steps of 1/10 h2 = seq(0,1,.1) # the probability of getting heads is all possible values from 0/10 to 10/10 in steps of 1/10 # set out weightings w1 = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0) # set the weights w1 = w1 / sum(w1) # make sure weights sum to 1 w2 = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) # set the weights w2 = w2 / sum(w2) # make sure weights sum to 1 M = dbinom(h,n, h1) # generate the likelihood M0 = sum(M * w1) # take a weighted average of the likelihood M1 = sum(M * w2) # take a weighted average of the likelihood glue::glue(&quot;The likelihood under $\\\\mathcal{{M}}_0$ is {round(M0,2)} The likelihood under $\\\\mathcal{{M}}_1$ is {round(M1,2)} The likelihood ratio is {round(M0/M1,2)} The data are {round(M0/M1,2)} times more probable under $\\\\mathcal{{M}}_0$ than $\\\\mathcal{{M}}_1$ The data are {round(M1/M0,2)} times more probable under $\\\\mathcal{{M}}_1$ than $\\\\mathcal{{M}}_0$&quot;) The likelihood under \\(\\mathcal{M}_0\\) is 0.04 The likelihood under \\(\\mathcal{M}_1\\) is 0.08 The likelihood ratio is 0.53 The data are 0.53 times more probable under \\(\\mathcal{M}_0\\) than \\(\\mathcal{M}_1\\) The data are 1.87 times more probable under \\(\\mathcal{M}_1\\) than \\(\\mathcal{M}_0\\) Now that we’re adding in these weighting priors, the ratio of \\(\\frac{\\mathcal{M}_1}{\\mathcal{M}_2}\\) is usually called a Bayes factor. Once we make these weightings explicit by adding a prior we can see the that for a particular observation or set of data, the ratio, or Bayes factor is completely dependent on our choice of the prior for \\(\\mathcal{M}_0\\) and \\(\\mathcal{M}_1\\). This is why it sometimes makes more sense to think of priors as models of hypotheses. And our Bayes factor tells us which model is better supported by the data. We can see this dependence in the example below. In each of the four quadrants, I’ve held the Data (\\(\\mathbf{y}\\)) and model that represents the fair coin (\\(\\mathcal{M}_0\\)) constant. However, I’ve simply varied \\(\\mathcal{M}_1\\). Each one of these \\(\\mathcal{M}_1\\)’s presents different views of what it means for a coin to be unfair. Each plot shows the distribution of weightings and the resulting Bayes factor. As you can see, when the prior changes the Bayes factor changes. # 12 # define function for calculating bayes factor for coin flip example # function takes 4 inputs # n_heads: the number of heads observed # n_flips: the number of flips performed # A : the first shape parameter of the beta distribution prior # B : the second shape parameter of the beta distribution prior bf_func&lt;-function(n_heads,n_flips,A,B){ t = 1/n_flips # set the step size of the theta range theta_range = seq(0.01,.99,length.out = n_flips + 1) # set the range of thetas theta_range = unique(c(0.5, theta_range)) # make sure the theta range contains 0.5! (for the null) w1 = map_dbl(theta_range, function(x) ifelse(x == 0.5, 1, 0)) # set the weights of hypothesis 1 (null) w2 = (dbeta(x = theta_range,A,B)) / # set weights of hypothesis 2 (alternative) using a sum(dbeta(x = theta_range,A,B)) # beta distribution Likelihood = dbinom(n_heads,n_flips,theta_range) # define the likelihood function M1 = Likelihood * w1 # multiply the likelihood by prior 1 M2 = Likelihood * w2 # multiply the likelihood by prior 2 BF = (sum(M1) / sum(M2)) # work out the bayes factor dat = tibble(x = theta_range, y = w2) # prepare data for ploting ggplot(data = dat, aes(x = x, y = y)) + geom_line() + geom_vline(xintercept = n_heads/n_flips, linetype = 2) + # mark the observation labs(x = &quot;θ&quot;, y = &quot;p(θ)&quot;, title = glue::glue(&quot;BF M1/M2 = {round(BF,2)}&quot;), subtitle = glue::glue(&quot;p ~ Beta({A},{B})&quot;)) + theme_minimal(15) # plot the prior } n_heads = 8; n_flips = 10; # set the observation bf_func(n_heads,n_flips,1,1) + bf_func(n_heads,n_flips,.5,.5) + bf_func(n_heads,n_flips,.5,1) + bf_func(n_heads,n_flips,10,10) %&gt;% wrap_plots() + plot_annotation(title = glue::glue(&quot;BFs for observing {n_heads} heads in {n_flips} flips&quot;)) This fact shouldn’t be alarming. A Bayes factor is going to tell you whether your data provide evidence for one model over another model. Change the models, and your data will say something different. This also highlights that the two hypotheses/models always have to be specified. There’s no such thing as the null hypothesis or the alternative hypothesis—only the two that have been specified. Because Bayes factors are dependent on priors, it pays to think carefully about them. There are many schools of thought about how one should go about formulating priors. Some of these might include: Choosing priors that reflect our genuine beliefs about the state of the world. For example, our beliefs about what it means for a coin to be unfair. Choosing priors that reflect our ignorance about the state of the world. For example, our belief that we have no grounds for favouring one parameter value (sub-hypothesis) over another. We might view this as giving the data the best opportunity to speak for itself. Choosing priors that have specific (or useful) mathematical properties. Choosing priors that have been tuned such that our Bayes factor tests will have good frequentist properties—for example, that they produce BFs that is some sense line up with p-values This, again, is not meant to be an exhaustive list. People might have other reasons for choosing the priors they do. And some prior choice might have multiple motivations. But this rough categorisation will do for our purposes. In the next section, we’ll examine some of these strategies so that we can understand their implications. And we’ll also try our hand at calculating Bayes factors for some data of the sort we get in psychology experiments (rather than silly coin flips!). But before we get to that, let’s cover something that we haven’t covered yet—Bayes Rule. "],
["bayes-rule.html", "Chapter 4 Bayes rule 4.1 What is Bayes rule 4.2 Bayes factor", " Chapter 4 Bayes rule I’ve left talking about Bayes rule until now, because I think you can understand the concept of the Bayes factor without it, and because I wanted to emphasise the idea that the Bayes factor is a ratio of two weighted averages. However, now that we have this simple understanding, I’m hoping to deepen your understanding a bit by introducing Bayes rule. This deeper understanding of Bayes rule will also help use understand some of the topic we’ll cover later in the course. 4.1 What is Bayes rule Bayes rule follows straightforwardly from the axioms of conditional probability. In this sense, there’s nothing particularly “Bayesian” about it in that both Frequentists and Bayesians can, and do, make use of the concept of conditional probability. 4.1.1 Conditional probability form When you encounter Bayes rule in a frequentist context, it often takes the following form: \\[p(A|B) = \\frac{p(B|A)p(A)}{p(B)}\\] or \\[p(A|B) = \\frac{p(B|A)p(A)}{p(B|A)p(A) + p(B|\\neg{}A)p(\\neg{}A)}\\] In this form, it typically explained by way of an example usually involving some kind of a test. In classic examples, the context if often a test for a rare disease. It is then shown that Bayes rule can be used to calculate the probability that the positive test indicates the presence of the disease [p(disease present| positive test)], by taking into account the sensitivity of the test [p(positive test | disease present)], the prevalence of the disease [p(disease)], and the probability of the test returning a positive result irrespective of the presence of the disease [p(positive)]. Bayes rule presented in this form is useful for thinking about evidence. The left side of the equation - \\(\\frac{p(B|A)p(A)}{p(B)}\\) - or more specifically, part of it - \\(\\frac{p(B|A)}{p(B)}\\) - can be read as representing the evidence the test provides or the presence of the disease. This evidence is then weighted by the base rate or the prior probability of the disease being present. 4.1.2 Proportional form In the context of Bayesian inference, it is often given in a slightly different form: \\[p(A|B) \\propto{} P(B|A) \\cdot{} P(A)\\] or \\[p(\\theta|Y) \\propto{} \\mathcal{L}(\\theta|Y) \\cdot{}p(\\theta)\\] In this form it is usually read as “the posterior probability is proportional to the likelihood times the prior”. The proportional form drops the denominator, which for a continuous parameter is given as: \\[p(Y) = \\int_\\Theta p(Y|\\theta)p(\\theta)d(\\theta)\\] Integrals are generally difficult to work out, so they’re often best avoided! We’ll see in the section on parameter estimation that while it’s not always possible to work out the posterior, we can just draw samples from it without needing to solve the integral. 4.1.3 Ratio form Both of these forms, however, obscure the relationship between Bayes and prediction. Following Rouder and Morey (2019), I think it’s useful to present Bayes rule in the ratio form: \\[\\frac{\\pi(\\theta|Y)}{\\pi({\\theta})}=\\frac{p(Y|\\theta)}{p(Y)}\\] The ratio form relates our “beliefs” about parameters \\(\\frac{\\pi(\\theta|Y)}{\\pi({\\theta})}\\) to probabilities about data \\(\\frac{p(Y|\\theta)}{p(Y)}\\). Or put another way, it relates beliefs and evidence to predictions. To understand how this is the case, we’ll examine the example given by Rouder and Morey (2019). To explore this formula we’ll first have to set two things. First, we’ll need to set what our observation is—that is, our data. This will just be the number of heads (\\(x\\)) we’ve observed after \\(n\\) flips. The second thing we need to set if our prior. This is just the weights that we set in the previous section, and the prior represents our “beliefs” about plausible values for the parameter (in our case, the bias of the coin) before seeing the data (more on whether priors represent beliefs in the next section). We’ll represent our prior with a \\(\\mathbf{Beta}\\) distribution, because this has some convenient mathematical properties (again, more on that in the next section). By changing the two parameters of the \\(\\mathbf{Beta}\\) distribution (\\(\\alpha\\) and \\(\\beta\\)) you can assign more or less prior mass to the extreme (i.e., \\(\\theta\\) = 0 and \\(\\theta\\) = 1). When the values are the same, the distribution will be symmetrical and then they’re different the distribution with be asymmetrical. For our simple coin flip example, we’ll just be able to calculate the posterior directly. This posterior represents what we believe about the parameter after seeing the data. # 1 # Set the observation # This takes two parameters # X: The number of heads # N: The number of flips X = 2 N = 10 # set the prior # We&#39;ll use a Beta distribution as our prior # The beta distribution takes two parameters (α [alpha_prior] and β [beta_prior]) alpha_prior = 3 beta_prior = 3 # calculate summary of prior prior_mean = alpha_prior / (alpha_prior + beta_prior) prior_mode = case_when(alpha_prior == beta_prior &amp; alpha_prior == 1 ~ &quot;any value in (0,1)&quot;, alpha_prior == beta_prior &amp; alpha_prior &lt;1 ~ &quot;bimodal {0,1}&quot;, alpha_prior &lt;= 1 &amp; beta_prior &gt; 1 ~ &quot;0&quot;, alpha_prior &gt; 1 &amp; beta_prior &lt;= 1 ~ &quot;1&quot;, TRUE ~ round((alpha_prior - 1) / (alpha_prior + beta_prior - 2),2) %&gt;% as.character() ) prior_var = (alpha_prior * beta_prior) / ( ((alpha_prior + beta_prior)^2) * (alpha_prior + beta_prior + 1) ) glue(&quot;Our data is {X} heads in {N} flips&quot;) Our data is 2 heads in 10 flips glue(&quot;The prior is a $\\\\mathrm{{Beta}}$({alpha_prior}, {beta_prior}) distribution The mean (expected value): $\\\\theta$ = {round(prior_mean,2)} The mode (max probablity density): $\\\\theta$ = {prior_mode} The variance: {round(prior_var,3)}&quot;) The prior is a \\(\\mathrm{Beta}\\)(3, 3) distribution The mean (expected value): \\(\\theta\\) = 0.5 The mode (max probablity density): \\(\\theta\\) = 0.5 The variance: 0.036 # 2 # define a function for the prior prior_func = function(theta, alpha_prior,beta_prior){ dbeta(x = theta, shape1 = alpha_prior, shape2 = beta_prior)} # create a tibble to hold the plot data prior_df = tibble(theta = seq(0,1,.01)) %&gt;% mutate(density = map_dbl(theta, function(x) prior_func(theta = x, alpha_prior, beta_prior))) # 3 # calculate the posterior # because the beta distribution is a conjugate prior for the binomial # we can directly compute the posterior, which will also be a # Bet distribution # define a function for the posterior posterior_func = function(theta, X, N, alpha_prior, beta_prior){ dbeta(x = theta, shape1 = alpha_prior + X, shape2 = beta_prior + N - X) } # create a tibble to hold the plot data posterior_df = tibble(theta = seq(0,1,.01)) %&gt;% mutate(density = map_dbl(theta, function(x) posterior_func(theta = x, X, N, alpha_prior, beta_prior))) # calculate summary of posterior alpha_posterior = alpha_prior + X beta_posterior = beta_prior + N - X posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior) posterior_mode = (alpha_posterior - 1) / (alpha_posterior + beta_posterior - 2) posterior_var = (alpha_posterior * beta_posterior) / ( ((alpha_posterior + beta_posterior)^2) * (alpha_posterior + beta_posterior + 1) ) glue(&quot;The posterior is a $\\\\mathrm{{Beta}}$({alpha_posterior}, {beta_posterior}) distribution The mean (expected value): $\\\\theta$ = {round(posterior_mean,2)} The mode (max probablity density): $\\\\theta$ = {round(posterior_mode,2)} The variance: {round(posterior_var,3)}&quot;) The posterior is a \\(\\mathrm{Beta}\\)(5, 11) distribution The mean (expected value): \\(\\theta\\) = 0.31 The mode (max probablity density): \\(\\theta\\) = 0.29 The variance: 0.013 # 4 # plot the prior and posterior combined_df = prior_df %&gt;% full_join(posterior_df, by = &#39;theta&#39;, suffix = c(&quot;.prior&quot;,&quot;.posterior&quot;)) %&gt;% mutate(`prior &gt; posterior` = density.prior &gt; density.posterior) %&gt;% pivot_longer(cols = c(&quot;density.prior&quot;,&quot;density.posterior&quot;), names_to = &quot;type&quot;, values_to = &quot;density&quot;) ggplot(combined_df, aes(x = theta, y = density, colour = type, linetype = type)) + geom_line(size = 1) + scale_colour_manual(values = c(density.posterior = &quot;seagreen&quot;, density.prior = &quot;darkblue&quot;), labels = c(density.posterior = &quot;posterior&quot;, density.prior = &quot;prior&quot;), name = NULL) + scale_linetype(guide = &quot;none&quot;) + geom_point(data = combined_df %&gt;% group_by(`prior &gt; posterior`) %&gt;% mutate(mid = abs(median(theta) - theta)) %&gt;% filter(mid == min(mid)) %&gt;% slice(1:2), mapping = aes(x = theta, y = density, colour = type, shape = `prior &gt; posterior`), fill = &quot;white&quot;, size = 6) + scale_shape_manual(values = c(&quot;TRUE&quot; = 21, &quot;FALSE&quot; = 19), guide = &quot;none&quot;) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) + theme(legend.position = &quot;top&quot;) Once we plot the prior and the posterior together we’ll see that for some values of \\(\\theta\\) seeing the data resulted in us believing that that value of \\(\\theta\\) is more probable. For other values, we now believe that that value of \\(\\theta\\) is less probable (in the plots, a value that is less probable after seeing the data is shown with empty point and a value that is more probable after seeing the data is shown with a filled point). For each value of the parameter we can examine whether the data resulted in us believing that that value of the parameter is more or less probable. We can call this the strength of evidence from the data about \\(\\theta\\). We can calculate this by just calculating the relative difference between the prior and the posterior—that is, by calculating \\(\\frac{\\pi(\\theta|Y)}{\\pi(\\theta)}\\). # 5 # strength of evidence for each value of theta updating_df = combined_df %&gt;% pivot_wider(names_from = &quot;type&quot;, values_from = &quot;density&quot;) %&gt;% mutate(updating = density.posterior / density.prior) ggplot(updating_df, aes(x = theta, y = updating)) + geom_line(na.rm = TRUE, size = 1) + geom_point(data = updating_df %&gt;% group_by(`prior &gt; posterior`) %&gt;% mutate(mid = abs(median(theta) - theta)) %&gt;% filter(mid == min(mid)) %&gt;% slice(1), mapping = aes(x = theta, y = updating, shape = `prior &gt; posterior`), fill = &quot;white&quot;, size = 6) + scale_shape_manual(values = c(&quot;TRUE&quot; = 21, &quot;FALSE&quot; = 19), guide = &quot;none&quot;) + scale_y_continuous(name = &quot;strength of evidence (updating factor)&quot;) + geom_hline(yintercept = 1, linetype = 2) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) We can now turn our attention to the data and we can ask: “what is the probability of different observations assuming different values of \\(\\theta\\)?”. This can be done with a simulation (like in our earlier examples); however, I know that it follows a \\(\\mathbf{Binomial}\\) distribution, so I can just generate it for different assumed values of \\(\\theta\\). # 6 thetas = updating_df %&gt;% group_by(`prior &gt; posterior`) %&gt;% mutate(mid = abs(median(theta) - theta)) %&gt;% filter(mid == min(mid)) %&gt;% slice(1) %&gt;% arrange(theta) %&gt;% select(`prior &gt; posterior`,theta) conditional_df = map_df(thetas$theta, function(t) tibble(x = 0:N, prob = dbinom(x = 0:N, size = N, prob = t), theta = t)) %&gt;% mutate(ob = x == X) %&gt;% group_split(theta) conditional_plots = map(conditional_df, function(d) ggplot(data = d, aes(x = x, y = prob)) + geom_point(aes(alpha = ob), size = 6) + geom_line(alpha = .1) + scale_alpha_manual(values = c(.1,1), guide = &quot;none&quot;) + scale_x_continuous(breaks = seq(0,N,2), name = &quot;number of heads&quot;) + scale_y_continuous(name = &quot;p(Y|θ)&quot;) + labs(title = glue(&quot;θ = {d$theta[1]}&quot;)) + theme_minimal(12)) patchwork::wrap_plots(conditional_plots,nrow = 2) The next concept, \\(p(Y)\\), or the marginal probability, is a slightly tricky concept: \\(p(Y)\\) is the probability of observing our data independent of whatever value \\(\\theta\\) might take. Often this value is ignored, especially in the context of parameter estimation (as you’ll see in later sections). In fact, this value isn’t present in the “proportional” formulation of Bayes rule; however, understanding \\(p(Y)\\) is extremely useful in the context of Bayes factors. The marginal probability distribution/mass plot can be more readily conceptualised as the predictions a model (\\(\\mathcal{M}_I\\)) makes about the data. We can generate this by seeing what data is predicted by each value of \\(\\theta\\) where \\(\\theta\\) itself has a probability distribution specified by \\(\\pi(\\theta)\\). This concept is maybe easiest to understand when we consider a uniform prior where each value of \\(\\theta\\) is equally probably. Then we can ask, what is the probability of observing a specific outcome \\(Y\\) independent of the value of \\(\\theta\\) (or, averaged across all possible values of \\(\\theta\\). This is just \\(\\frac{1}{n}\\), where \\(n\\) is the number of possible outcomes. In our coin flip example, there are 11 possible outcomes—0 heads, 1 head, 2 heads,… 10 heads. So \\(p(Y)\\) would be \\(\\frac{1}{11}\\) for any outcome. Or phrased another way, we can say that, without knowing \\(\\theta\\), but knowing that every value of \\(\\theta\\) is equally probably, we can predict that any observation, such as our specific observation, would occur with a probability of \\(\\frac{1}{11}\\). A very important thing to note about the marginal probability distribution is that it must sum to 1. We’ll see in the example below, that for different priors (\\(\\pi(\\theta)\\)), the pattern see in the marginal distribution changes, but it always sums to 1. This means that when some observations become more probable, other observations must become less probable. In the table below, you’ll see how the marginal probability is calculated for each observation. The table just shows the calculation for our specific observation—that is, our \\(p(Y)\\). Note that the accuracy of our estimate for \\(p(Y)\\) depends on how many values of \\(\\theta\\) we average across. This means that for a uniform prior, the limit of our estimate will approach \\(\\frac{1}{11}\\) when the number of values of \\(\\theta\\) that we average across approaches infinity. # 7 resolution = 1001 # set the resolution - this determines how many possible values of theta are considered. # to get an accurate estimate of theta, you need to consider every possible value of theta # adjust the value highter to get a more accurate estimate of p(Y) theta_range = seq(0,1,length.out = resolution) marginal_prob_tbl = tibble(theta = theta_range, x = X, n = N, cond_prob = map_dbl(theta_range, function(t) dbinom(x = X,size = N,prob = t)), prior_prob = map_dbl(theta_range, function(t) dbeta(x = t, shape1 = alpha_prior, shape2 = beta_prior))) %&gt;% mutate(prior_prob = prior_prob / sum(prior_prob)) marginal_prob_tbl = marginal_prob_tbl %&gt;% mutate(margin = cond_prob * prior_prob) marginal_prob_tbl %&gt;% slice(seq(1,resolution,length.out = 11)) %&gt;% set_colnames(c(&quot;$\\\\theta$&quot;,&quot;$X$&quot;,&quot;$N$&quot;,&quot;$p(Y|\\\\theta$)&quot;, &quot;$\\\\pi(\\\\theta)$&quot;,&quot;$p(Y,\\\\theta,\\\\pi)$&quot;)) %&gt;% knitr::kable(format = &quot;html&quot;, digits = 3) %&gt;% kableExtra::kable_styling(full_width = T) \\(\\theta\\) \\(X\\) \\(N\\) \\(p(Y|\\theta\\)) \\(\\pi(\\theta)\\) \\(p(Y,\\theta,\\pi)\\) 0.0 2 10 0.000 0.000 0 0.1 2 10 0.194 0.000 0 0.2 2 10 0.302 0.001 0 0.3 2 10 0.233 0.001 0 0.4 2 10 0.121 0.002 0 0.5 2 10 0.044 0.002 0 0.6 2 10 0.011 0.002 0 0.7 2 10 0.001 0.001 0 0.8 2 10 0.000 0.001 0 0.9 2 10 0.000 0.000 0 1.0 2 10 0.000 0.000 0 glue::glue(&quot;$p(Y)$ = {marginal_prob_tbl %&gt;% pull(margin) %&gt;% sum() %&gt;% round(3)}&quot;) \\(p(Y)\\) = 0.09 The table just shows the marginal probability for our observation, but in the figure below we can plot the marginal distribution which considers every possible observation. This allows us to look of the entire range of possible observations and see which are more or less probable. These are the predictions our model makes. # 8 marginal_func = function(theta,X,N,alpha_prior,beta_prior) dbinom(x = X, size = N, prob = theta) * dbeta(x = theta, shape1 = alpha_prior, shape2 = beta_prior) marginal_df = tibble(x = 0:N, marginal_prob = map_dbl(.x = 0:N, .f = function(x) integrate(f = marginal_func, lower = 0, upper = 1, x, N, alpha_prior, beta_prior)$value), ob = x == X) general_model_plot = marginal_df %&gt;% ggplot(aes(x = x, y = marginal_prob)) + geom_point(aes(alpha = ob), size = 6) + geom_line(alpha = .1) + scale_alpha_manual(values = c(.1,1), guide = &quot;none&quot;) + scale_x_continuous(breaks = seq(0,N,2), name = &quot;number of heads&quot;) + scale_y_continuous(name = &quot;marginal probability&quot;) + theme_minimal(12) general_model_plot We can compare the marginal probability of our observation \\(p(Y)\\) with the conditional probability \\(p(Y|\\theta)\\) — that is, conditional on a specific value of \\(\\theta\\). The ratio of these two \\(\\frac{p(Y|\\theta)}{p(Y)}\\) is the predictive accuracy for our data that gained by considering \\(\\theta\\). The following plot simply shows the conditional probability of the data give different values of the paramater (labelled conditional) and the marginal probability or the probability of the data irrespective of the value of the parameter (labelled marginal). # 9 marginal_conditional_df = tibble(theta = c(seq(0,1, .001),thetas$theta), prob = dbinom(x = X, size = N, prob = c(seq(0,1,.001),thetas$theta))) %&gt;% mutate(highlight = theta %in% thetas$theta) %&gt;% full_join(thetas, by = &quot;theta&quot;) marginal_prob = marginal_prob_tbl %&gt;% pull(margin) %&gt;% sum(na.rm = T) marginal_conditional_df$marginal_prob = marginal_prob marginal_conditional_df %&gt;% ggplot(aes(x = theta, y = prob)) + geom_line(aes(colour = &quot;conditional&quot;), size = 1) + geom_line(aes(x = theta, y = marginal_prob, colour = &quot;marginal&quot;), linetype = 2, size = 1) + geom_point(. %&gt;% filter(highlight == TRUE), mapping = aes(x = theta, y = prob, shape = `prior &gt; posterior`), size = 6, colour = &quot;darkblue&quot;, fill = &quot;white&quot;) + geom_point(. %&gt;% filter(highlight == TRUE), mapping = aes(x = theta, y = marginal_df %&gt;% filter(x == X) %&gt;% pull(marginal_prob), shape = `prior &gt; posterior`), size = 6, fill = &quot;white&quot;, colour = &quot;seagreen&quot;) + scale_shape_manual(values = c(&quot;TRUE&quot; = 21, &quot;FALSE&quot; = 19), guide = &quot;none&quot;) + scale_y_continuous(glue(&quot;Pr({X} heads in {N} flips)&quot;)) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) + scale_color_manual(values = c(&quot;darkblue&quot;,&quot;seagreen&quot;), name = NULL) # 10 marginal_conditional_df %&gt;% mutate(marginal = marginal_df %&gt;% filter(x == X) %&gt;% pull(marginal_prob)) %&gt;% mutate(predictive = prob / marginal) %&gt;% ggplot(aes(x = theta, y = predictive)) + geom_line(size = 1) + geom_point(. %&gt;% filter(highlight == TRUE), mapping = aes(x = theta, y = predictive, shape = `prior &gt; posterior`), size = 6, fill = &quot;white&quot;) + scale_shape_manual(values = c(&quot;TRUE&quot; = 21, &quot;FALSE&quot; = 19), guide = &quot;none&quot;) + scale_y_continuous(&quot;gain in predictive accuracy&quot;) + geom_hline(yintercept = 1, alpha = .1) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(base_size = 18) This plot is just the same as the strength of evidence for values of \\(\\theta\\) or the factor by which we update our beliefs about \\(\\theta\\) after observing the data. This fact is just represented by the equality in the ratio form of Bayes rule \\(\\frac{\\pi(\\theta|Y)}{\\pi({\\theta})}=\\frac{p(Y|\\theta)}{p(Y)}\\). This equation can now we read as meaning that the strength of evidence that we have for a parameter value is just the same as the gain in predictive accuracy. 4.2 Bayes factor In this example, we’ve only considered one model defined by the prior we set at the beginning. However, marginal densities are particularly useful when we consider multiple models. In the next example, we plot the marginal density for our current model (\\(\\mathcal{M}_1\\); subplot A) and more restricted model where we no longer a probability distribution over every possible value of \\(\\theta\\), but instead only consider one possible value, \\(\\theta\\) = 0.5 (\\(\\mathcal{M}_2\\); subplot A). The difference in predictions the models make is shown in subplot C. This plot is just generated as the ratio \\(\\frac{p(Y|\\mathcal{M}_1)}{p(Y|\\mathcal{M}_2)}\\). Once we have our data in hand, we can see whether our data is better predicted by Model 1 or Model 2—this value is the Bayes factor. # 11 marginal_df_null = tibble(x = 0:N, marginal_prob = map_dbl(.x = 0:N, .f = function(x) dbinom(x = x, size = N, prob = 0.5) * 1), ob = x == X) null_plot = marginal_df_null %&gt;% ggplot(aes(x = x, y = marginal_prob)) + geom_point(aes(alpha = ob), size = 6) + geom_line(alpha = .1) + scale_alpha_manual(values = c(.1,1), guide = &quot;none&quot;) + scale_x_continuous(breaks = seq(0,N,2), name = &quot;number of heads&quot;) + scale_y_continuous(name = &quot;p(Y)&quot;) + theme_minimal(12) bf_plot = marginal_df %&gt;% full_join(marginal_df_null, by = c(&quot;x&quot;,&quot;ob&quot;), suffix = c(&quot;.general&quot;,&quot;.null&quot;)) %&gt;% mutate(BF = marginal_prob.general / marginal_prob.null) %&gt;% ggplot(aes(x = x, y = BF)) + geom_point(aes(alpha = ob), size = 6) + geom_line(alpha = .1) + scale_alpha_manual(values = c(.1,1), guide = &quot;none&quot;) + scale_x_continuous(breaks = seq(0,N,2), name = &quot;number of heads&quot;) + scale_y_continuous(name = &quot;bayes factor&quot;, trans = &quot;log10&quot;) + geom_hline(yintercept = 1) + theme_minimal(12) models = ((general_model_plot + labs(title = &quot;general model&quot;)) / null_plot + labs(title = &quot;restricted model&quot;)) p_ranges_y &lt;- c(ggplot_build(models[[1]])$layout$panel_scales_y[[1]]$range$range, ggplot_build(models[[2]])$layout$panel_scales_y[[1]]$range$range) models = suppressMessages(models &amp; ylim(min(p_ranges_y), max(p_ranges_y)) &amp; ylab(&quot;prob of outcome&quot;)) ((models | (bf_plot + labs(title = &quot;difference in prediction&quot;) )) + plot_annotation(tag_levels = &quot;A&quot;)) &amp; theme(title = element_text(size = 14)) # 12 # Give the Bayes factors for different observations marginal_df %&gt;% full_join(marginal_df_null, by = c(&quot;x&quot;,&quot;ob&quot;), suffix = c(&quot;.general&quot;,&quot;.null&quot;)) %&gt;% mutate(BF10 = marginal_prob.general / marginal_prob.null, BF01 = 1/ BF10, n = max(x), y = glue::glue_col(&quot;{x} in {n}&quot;)) %&gt;% select(x,n,y,marginal_prob.general, marginal_prob.null, BF10, BF01) %&gt;% set_colnames(c(&quot;heads&quot;,&quot;flips&quot;,&quot;$Y$&quot;, &quot;$p(Y|\\\\mathcal{M}_1)$&quot;, &quot;$p(Y|\\\\mathcal{M}_0)$&quot;,&quot;$BF_{10}$&quot;,&quot;$BF_{01}$&quot;)) %&gt;% knitr::kable(digits = 2, align = &quot;c&quot;) %&gt;% kableExtra::kable_styling(full_width = T) heads flips \\(Y\\) \\(p(Y|\\mathcal{M}_1)\\) \\(p(Y|\\mathcal{M}_0)\\) \\(BF_{10}\\) \\(BF_{01}\\) 0 10 0 in 10 0.02 0.00 22.51 0.04 1 10 1 in 10 0.05 0.01 5.63 0.18 2 10 2 in 10 0.09 0.04 2.05 0.49 3 10 3 in 10 0.12 0.12 1.02 0.98 4 10 4 in 10 0.14 0.21 0.68 1.47 5 10 5 in 10 0.15 0.25 0.60 1.68 6 10 6 in 10 0.14 0.21 0.68 1.47 7 10 7 in 10 0.12 0.12 1.02 0.98 8 10 8 in 10 0.09 0.04 2.05 0.49 9 10 9 in 10 0.05 0.01 5.63 0.18 10 10 10 in 10 0.02 0.00 22.51 0.04 "],
["choosing-priors-part-i.html", "Chapter 5 Choosing priors Part I 5.1 Priors with specific mathematical properties 5.2 Priors that reflect ignorance 5.3 Priors that reflect our beliefs about the world.", " Chapter 5 Choosing priors Part I We’re going to cover priors in two parts. In this part, we’re going to cover some general implications of different prior choices and we’re going to talk about priors specifically in the context of Bayes factor. In Part II, we cover prior choice in the context of Bayesian estimation. The reasons for making particular choices and the implications of our choices in these two contexts are a little different, so it’s best to deal with them separately. 5.1 Priors with specific mathematical properties The first prior choice strategy we’ll cover is choosing priors that have particular mathematical properties. One property we might be interested in is choosing priors that are from the same family as the posterior. When we do this, the prior is known as a conjugate prior. Ordinarily there aren’t closed form solutions for calculating posteriors. However, the beauty of using a conjugate prior is that this allows us to derive a closed form solution for the posterior. One example of a conjugate prior is the Beta distribution, which is a conjugate prior when we have a Binomial or Negative binomial likelihood (like our coin flip example). The resulting posterior would then take the form of a Beta distribution. # 1 # Set the observation # This takes two parameters # X: The number of heads # N: The number of flips X = 5 N = 12 # set the prior # We&#39;ll use a Beta distribution as our prior # The beta distribution takes two parameters (α [alpha_prior] and β [beta_prior]) alpha_prior = 10 beta_prior = 1 # define a function for the likelihood like_func = function(theta, x,n){ dbinom(x = x, size = n, prob = theta)} # create a tibble to hold the plot data like_df = tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(x) like_func(theta = x, X, N))) # define a function for the prior prior_func = function(theta, alpha_prior,beta_prior){ dbeta(x = theta, shape1 = alpha_prior, shape2 = beta_prior)} # create a tibble to hold the plot data prior_df = tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(x) prior_func(theta = x, alpha_prior, beta_prior))) # define a function for the posterior posterior_func = function(theta, X, N, alpha_prior, beta_prior){ dbeta(x = theta, shape1 = alpha_prior + X, shape2 = beta_prior + N - X) } # create a tibble to hold the plot data posterior_df = tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(x) posterior_func(theta = x, X, N, alpha_prior, beta_prior))) like_df$density = like_df$density * (max(prior_df$density)/max(like_df$density)) combined_df = prior_df %&gt;% full_join(posterior_df, by = &quot;theta&quot;) %&gt;% full_join(like_df, by = &quot;theta&quot;) %&gt;% set_colnames(c(&quot;theta&quot;,&quot;prior&quot;,&quot;posterior&quot;,&quot;likelihood&quot;)) %&gt;% pivot_longer(cols = c(&quot;prior&quot;,&quot;posterior&quot;,&quot;likelihood&quot;), names_to = &quot;type&quot;, values_to = &quot;density&quot;) ggplot(combined_df, aes(x = theta, y = density, colour = type, linetype = type)) + geom_line(size = 1, na.rm = T) + scale_colour_manual(values = c(posterior = &quot;seagreen&quot;, prior = &quot;darkblue&quot;, likelihood = &quot;red&quot;), name = NULL) + scale_linetype(guide = &quot;none&quot;) + scale_y_continuous(name = NULL, labels = NULL) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) + theme(legend.position = &quot;top&quot;) 5.2 Priors that reflect ignorance The next thing that we might want to take into account when choosing a prior is choosing a prior that doesn’t commit us to any particular value of the parameter. But doing this might not be as straightforward as it initially seems. We might think that we can just weight each possible value of the parameter equally. However, it turns out that this isn’t always the best strategy. To see why, we’ll turn to an example. For our coin flipping example we can calculate the maximum likelihood estimate for the coin bias given our data. The formula for this is given as: \\[\\hat{\\theta}_{\\mathrm{mle}}=\\frac{x}{n},\\] where \\(x\\) is the number of heads in \\(n\\) flips. This value is the same as the mean of our likelihood. We can think of this as our estimate of the parameter without taking account any prior information. Next we can incorporate a prior and calculate a posterior. We’ll use a Beta prior, because it’ll allow us to easily calculate the posterior. For a prior of the form \\(Beta(\\alpha,\\beta)\\), the posterior for \\(x\\) heads in \\(n\\) flips is given as \\(Beta(\\alpha + x,\\beta + n -x)\\). From this, we can now work out the mean of the posterior as: \\[\\hat{\\theta}_{\\mathrm{bayes}}=\\frac{\\alpha + x}{\\alpha + \\beta + n}.\\] From this we can see that for any value of \\(\\alpha &gt; 0 &lt; \\beta\\) the value of \\(\\hat{\\theta}_{\\mathrm{bayes}}\\) and \\(\\hat{\\theta}_{\\mathrm{mle}}\\) will be different. This results in a rather unintuitive conclusion. The uniform prior \\(Beta(1,1)\\) has a bigger influence (is in a sense more informative) than the prior \\(Beta(0,0)\\) (this prior is called Haldane’s prior). Technically, \\(Beta(0,0)\\) is undefined, but we can examine the prior \\(Beta(\\epsilon,\\epsilon)\\), where \\(\\epsilon\\) is an arbitrarily small number. # 2 # Set the observation # This takes two parameters # X: The number of heads # N: The number of flips X = 5 N = 12 # set the prior # We&#39;ll use a Beta distribution as our prior # The beta distribution takes two parameters (α [alpha_prior] and β [beta_prior]) alpha_prior = 0.01 beta_prior = 0.01 glue(&quot;The mean of the likelihood is {round(X/N,2)} The mean of the posterior is {round((alpha_prior + X)/(alpha_prior + beta_prior + N),2)}&quot;) The mean of the likelihood is 0.42 The mean of the posterior is 0.42 tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(x) prior_func(theta = x, alpha_prior, beta_prior))) %&gt;% ggplot(aes(x = theta, y = density)) + geom_line(size = 1) + scale_y_continuous(name = NULL, labels = NULL) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) From the plot we can see that this prior splits nearly all of it’s weight on the extreme of \\(\\theta\\) = 0 and \\(\\theta\\) = 1. In short, a prior that seems to make the least commitments has a bigger influence than the prior that seemingly makes the strongest commitments. If we think about it, we’ll also see that a prior like this makes a lot of sense if we want to quantify our uncertainty. To understand this, lets plot the likelihood functions for various observations. We’ll pick two extreme observations (1 head in 100 flips and 99 heads in 100 flips) and one middle of the road observation (50 heads in 100 flips). The plots will allow us to see the range of plausible values for the coin bias given the observation. # 3 like_options = list(c(1,100), c(99,100), c(50,100)) list(tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(t) like_func(theta = t, like_options[[1]][1], n = like_options[[1]][2])), x = like_options[[1]][1], n = like_options[[1]][2]), tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(t) like_func(theta = t, like_options[[2]][1], n = like_options[[2]][2])), x = like_options[[2]][1], n = like_options[[2]][2]), tibble(theta = seq(0,1,.0001)) %&gt;% mutate(density = map_dbl(theta, function(t) like_func(theta = t, like_options[[3]][1], n = like_options[[3]][2])), x = like_options[[3]][1], n = like_options[[3]][2])) %&gt;% map(function(x) ggplot(data = x, aes(x = theta, y = density)) + geom_line() + labs(title = glue(&quot;Likelihood for {x$x} heads in {x$n} flips&quot;)) + theme_minimal(16) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + scale_y_continuous(name = NULL, labels = NULL)) %&gt;% patchwork::wrap_plots(ncol = 1) Comparing the plots for the extreme observations to the plot for the middling one we can see that there’s a larger range of plausible values of the bias when the observation is somewhere near the middle of the range. When the observation is at the extreme there’s only a very narrow range of values of the bias that could plausibly produce the observation. In a sense, we’re less uncertain about the extremes and more uncertain in the middle. Quantifying this uncertainty means putting more prior mass at the extremes (where we’re certain) and less in the middle (where more uncertain about the relation between parameters and observations). There’s also another hurdle to coming up with a prior that is “uninformative”, and this has to do with how we parametrise problems. Let’s say that I’m interested in computing a posterior estimate about the probability \\(\\pi\\) of a coin landing heads. I use a uniform prior so that for any value of \\(\\theta_n\\), \\(p(\\theta_1)=p(\\theta_2)\\). After working out my posterior I give you the estimate. You, however, are not interested in the probability \\(\\pi\\). Instead, you’re interested in the log odds \\(\\phi\\), so you transform \\(\\pi\\) into \\(\\phi\\) using \\(\\phi=\\mathrm{log}\\frac{\\pi}{1-\\pi}\\). Now let’s say another another statistician is analysing the same data, but they’ve decided to work in \\(\\phi\\) space from the get go. So to analyse the data, they first convert it into \\(\\phi\\), construct a prior so that for any \\(\\theta_n\\), \\(p(\\theta_1)=p(\\theta_2)\\). They then take the data, and the prior, to compute the posterior. It turns out that when this is done, the two estimates—the one converted from \\(\\pi\\) to \\(\\phi\\) and the done wholly in \\(\\phi\\) won’t agree. That is because what is uniform for the one isn’t uniform for the other. So which way of asking the question is the right one? Do you ask about \\(\\pi\\) or \\(\\phi\\)? The answer is that there is, and there shouldn’t, be a right way to ask the question. If the parameters are equivalent (there’s a one to one translation from one to the other) and to use a non-informative prior then your choice of parameterisation shouldn’t inform your posterior. There is, however, a way to construct prior that will allow you to get the same answer for the two questions. This is by using Jeffreys’ rules. The resulting prior is often called a Jeffreys’ prior. For our coin flipping example, the prior constructed from the parameter space of the probability of show heads turns out to be \\(Beta(\\frac{1}{2},\\frac{1}{2})\\). This property of Jeffreys’ priors is certainly useful. However, there’s no free lunch in statistics, and using Jeffreys’ priors involves a cost. The cost is that to construct a Jeffreys’ prior you need to take into account the universe of possible results. That is, the method of constructing Jeffrey’s priors takes into account data that only might occur. As a consequence, the Jeffreys prior for our two coin flipping example—flipping for 10 flips and flipping until 2 tails—will be different. The Jeffreys prior for \\(n\\) flips is \\(Beta(\\frac{1}{2},\\frac{1}{2}\\)). While the prior for the flipping until \\(x\\) tails is \\(Beta(1,\\frac{1}{2})\\). jeffrey1 = tibble(theta = seq(0,1,.001)) %&gt;% mutate(density = map_dbl(theta, function(x) prior_func(theta = x, .5, .5))) %&gt;% ggplot(aes(x = theta, y = density)) + geom_line(size = 1) + scale_y_continuous(name = &quot;p(θ)&quot;, labels = NULL) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) + labs(title = &quot;Jeffreys prior for binomial&quot;) jeffrey2 = tibble(theta = seq(0,1,.001)) %&gt;% mutate(density = map_dbl(theta, function(x) prior_func(theta = x, 1, .5))) %&gt;% ggplot(aes(x = theta, y = density)) + geom_line(size = 1) + scale_y_continuous(name = &quot;p(θ)&quot;, labels = NULL) + scale_x_continuous(name = &quot;θ&quot;, breaks = seq(0,1,.2)) + theme_minimal(12) + labs(title = &quot;Jeffreys prior for negative binomial&quot;) jeffrey1 / jeffrey2 5.3 Priors that reflect our beliefs about the world. Finally, you can choose priors that reflect your beliefs about the world. In the coin flipping example, this would involve picking a prior that reflects what you believe about fair and biased coins: Do biased coins show heads more often, tails more often, extreme values more often, and so on. When we’re using Bayes factors in experimental psychology, this would me picking priors that describe our scientific theories—that is, picking priors that makes predictions about data that are consistent with the predictions about data that our theories make. Do this is hard, but we’ll cover some strategies when we get to actually computing Bayes factors for some problems. But it’s also important to remember that you don’t need to restrict yourself to one model. You can have many models and you can see how changes in your model change your inferences. This is sometimes captured by the idea of robustness regions. However, it’s also important to note that if different reasonable priors lead to different conclusions then there might not be one answer to your problem. That is, it important to recognise that uncertainty in the nature of your models can lead to uncertainty in your conclusions. "],
["computing-bayes-factors-part-i.html", "Chapter 6 Computing Bayes factors Part I 6.1 Priors on effect sizes", " Chapter 6 Computing Bayes factors Part I We’ve already worked through computing a Bayes factor for our coin flip example. In this section, we’re going to work through computing BFs for the kinds of problems you ordinarily encounter in psychology. We’re going to start simple, and look at the kind of problems that you might have answered with a t-test. But before that, we need to go back to our equation for \\(\\mathcal{M}_H\\): \\[\\mathcal{M}_H = \\sum_{i=1}^{n}\\mathcal{L}_H(\\theta_i|\\mathbf{y})\\cdot{}p(\\theta_i)\\] This equation worked fine for our coin flip example. We had a discrete number of parameter values (or sub-hypotheses) so we could just average (i.e., sum and then multiply by \\(\\frac{1}{n}\\)) their likelihood values. But now we’re going to start working with problems where our sub-hypotheses are going to span a continuous range. So we’ll just update our equation to reflect this. The equation “works” the same way, but it will look a little different. Our new equation is as follows: \\[\\mathcal{M}_H = \\int_{\\theta\\in\\Theta_H}\\mathcal{L}_H(\\theta|\\mathbf{y})p(\\theta)d\\theta\\] The equation still does the same thing—that is, it represents a continuous average of likelihoods, over the entire parameter space (\\(\\Theta_H\\)), with the prior (\\(p\\)) serving as the weights. 6.1 Priors on effect sizes We’ll start with a problem that’s outlined in Rouder et al (2009, pg 232). We’ll work through recreating the analysis, so that we can better understand how it works and what it means. Rouder et al (2009) reports some summary stats from Grider and Malmberg (2008). Grider and Malmberg (2008) ran a study assessing whether participants were bettter at remembering emotional words or netural words. Usually, we’re not working off summary stats, so we’ll use those summary stats to generate some synthetic data that matches those characteristics. Grider and Malmberg claimed that emotional words were remembered better than neutral words: # 1 # lets generate some data to work with because usually we work with raw data and not summary stats # statistics reported by Grider and Malmberg (2008) in Rouder et al (2009, pg 232) # reported values! sample_size = 80 netural_words = 0.76 positive_words = 0.80 t_stat = 2.24 # calculate the rest from the reported values! mean_of_difference = positive_words - netural_words sd_of_difference = (mean_of_difference * sqrt(sample_size)) / t_stat # generate some data that matches those characteristics g_m_2008_data = mean_of_difference + sd_of_difference * scale(rnorm(n = sample_size, 0, 1)) glue(&quot;Accuracy for positive words was {positive_words} Accuracy for neutral words was {netural_words}&quot;) Accuracy for positive words was 0.8 Accuracy for neutral words was 0.76 Since we now have “raw” data, we might as well draw a few plots. # 2 # lets take a look at the data plot1 = ggplot(mapping = aes(x = c(&quot;neutral&quot;,&quot;positive&quot;), y = c(netural_words,positive_words))) + geom_col() + theme_minimal(16) + scale_y_continuous(name = &quot;recall accuracy&quot;, limits = c(0,1)) + scale_x_discrete(name = &quot;valence&quot;) plot2 = ggplot(mapping = (aes(x = g_m_2008_data))) + geom_histogram(bins = 30) + scale_x_continuous(&quot;difference in accuracy&quot;) + theme_minimal(16) plot1 + plot2 + plot_annotation(tag_levels = &quot;A&quot;) We’ll now move on to the analysis. We’ll do this in 3 steps. In step 1, we’ll just run a t test like Grider and Malmberg (2008) did. In step 2, we’ll run the default Bayes factor t-test using the R package developed Rouder et al. But so that we can really understand what’s going on, in step 3, we’re going to calculate our own Bayes factor using the stuff we’ve learned up until now! First do a t test and report the results! # 3 # lets do some analysis! # first, lets run a *t* test like Grider and Malmberg (2008) t_test_results = t.test(g_m_2008_data) # now we&#39;ll format them nicely! t_test_results %&gt;% tidy() %&gt;% glue::glue_data(&quot;*t* ({parameter}) = {round(statistic,2)}, *p* = {round(p.value,2)}&quot;) t (79) = 2.24, p = 0.03 Now use an R package to calculate a Bayes factor: # 4 # now run a BF analysis with &quot;default&quot; priors store_bought_bf = BayesFactor::ttestBF(g_m_2008_data, rscale = 1) store_bought_bf %&gt;% tidy() %&gt;% glue::glue_data(&quot;JZS Bayes factor = {round(BF01,2)}&quot;) JZS Bayes factor = 1.02 Let’s try figure out where that number comes from and what it means by reverse engineering it! So far we’ve learned that we need 4 things to build a BF 1. we’ll need to choose a parameter to make inferences about 2. We need a likelihood function which will describe how likely different parameter values are, given our data 3. We need a model of our Model 1 (we can call it \\(\\mathcal{H}_0\\)) 4. We need a model of our Model 2 (we can call it \\(\\mathcal{H}_1\\)) What do Rouder et al have to say about their choices? Rouder et al choose to make inferences about standardised effect sizes (e.g., \\(\\delta\\)). This is probably a good choice, because standardised effect sizes are easily compared. However, raw effect (i.e., in milliseconds or percentages) are sometimes more scientifically meaningful. But we’ll follow their choice now. We’ll also need a likelihood function. The likelihood is going to describe the likely values of \\(\\delta\\) given our data. So the most likely value will be our observed effect size, and values \\(\\delta\\) will be less likely as we move away from this value. Unlike our coin flipping case it’s pretty hard to do a simulation to derive the likelihood ourselves. But if we did, we’d see that it follows a t likelihood with N - 1 degrees of freedom. This is what Rouder et al use. Next, we’ll need a model of our model 1 or null hypothesis (ie., \\(\\mathcal{H}_0\\)). Following Rouder, we’ll pick the hypothesis that \\(\\delta\\) = 0 as our \\(\\mathcal{H}_0\\). Up until now, we’ve only picked a single point for our model 1, but we don’t need to. To define our model 1, we’ll just say that for all values of \\(\\delta \\neq\\) 0 will be weighted 0, and \\(\\delta\\) = 0 will be weighted 1. Finally, we need a model for our model 2 or alternative hypothesis (i.e., \\(\\mathcal{H}_1\\)). Here Rouder et al (2009) aim for a non-informative prior1. Their choice, it turns out, it a Cauchy distribution. They derive this from some mathematical considerations. This means that this prior is not characterising their beliefs about reasonable effect sizes. In fact, we’ll see that it puts a lot of weight on completely unreasonable values that no scientific theory would predict. But that isn’t there aim. They’re trying to commit to as little as possible with their prior. Now that we have all the bits we need, we can start building up our Bayes factor. 1Their exact model is slightly different, involving a prior on effect size and on the standard deviation, but we can ignore this complication for now. The first step is our effect size parameter. This is calculated as the sample mean scaled by the sample standard deviation. That is: \\[\\delta=\\frac{\\mathrm{mean}}{\\mathrm{sd}}\\] # 5 # calculate delta delta = mean(g_m_2008_data) / sd(g_m_2008_data) glue::glue(&quot;The observed effect size is {round(delta,2)}&quot;) The observed effect size is 0.25 The next step is our likelihood function. We’ll define it with t.lik function from the bayesplay package. It’ll be centred at our \\(\\delta\\) valued and will have N - 1 degrees of freedom. It is used as follows: data_model = t.lik(center = ... , df = ...) # 6 # Lets set up our likelihood # we&#39;ve already calculated the delta which will be the centre # so we only need the number of data point # calculate df df = length(g_m_2008_data) - 1 # define liklihood data_model = t.lik(center = delta, df = df) data_model Object of class likelihood Likelihood type: non-central t Parameters Center: 0.250439613479976 DF: 79 To help us get a better idea of the likelihood, we’ll plot it. We’ll also mark our observation, which should be the most likely parameter value. To plot the likelihood we just use the plot() function. As inputs, it takes the likelihood object and the range of \\(\\delta\\) values you want to plot over. E.g.: plot(data_model, theta = seq(from = -1, to = 1,by = 0.001)) # 7 # Let&#39;s plot the likelihood function # We&#39;ll make the plot and then style it with standard ggplot syntax plot(data_model, theta = seq(from = -1, to = 1,by = 0.001)) + scale_x_continuous(name = &quot;δ&quot;) + theme_minimal(12) + geom_vline(xintercept = delta, linetype = 2) Now that we have our likelihood’s we’ll need to define our two models. The syntax for defining models is a little clunky, but essentially we just need to give a weighting for every possible value of \\(\\delta\\). These should also (ideally) be proper probability distributions. Most of our priors are either going to be normal distributions, cauchy distributions, uniform distributions, t distributions or similar. Luckily R provides functions for these probability densities. They’re are dnorm(), dcauchy(), dunif(), and dt() respectively. The basic structure of specifying a prior is as follows: prior = function(theta.range) { list(func = function(theta) PRIOR_FUNCTION, theta.range = theta.range) } For our \\(\\mathcal{H}_0\\), we’ll use a point null (just as we’ve been using for our coin flips). Our point null just says \\(\\delta\\) = 0. # 8 # we&#39;ll start by defining the null prior. # this is, for all values other than delta = 0, set the weight to 0, and when # delta = 0, set the weight to 1 h0_model = function(theta.range){ list(func = function(theta) ifelse(theta == 0, 1, 0), theta.range = theta.range) } Now that we’ve defined our \\(\\mathcal{H}_0\\), we’ll define \\(\\mathcal{H}_1\\). Following Rouder et al we’ll use a cauchy distribution. It’s defined as follows: # 9 # now we define the &quot;alternative&quot; prior # this will be a standard cauchy distribution h1_model = function(theta.range){ list(func = function(theta) dcauchy(x = theta, location = 0, scale = 1), theta.range = theta.range) } We’ll also plot our prior, so we can get an idea of what it looks like. # 10 # plot the prior # first set the range of values for the plot theta.range = seq(-10,10,.1) # from -10 to +10 in steps 0.1 # then make a tibble with the data for the plot tibble(theta = theta.range, p = h1_model(theta.range)$func(theta.range)) %&gt;% ggplot(aes(x = theta, y = p)) + geom_line() + scale_x_continuous(name = &quot;δ&quot;) + scale_y_continuous(name = &quot;p(δ)&quot;) + theme_minimal(12) As you can see there’s still a fair bit of weight at values of \\(\\delta\\pm\\) 5. These are unreasonable values for an effect size, but this prior isn’t about Rouder et al’s beliefs about reasonable effect sizes, so it’s not a concern to them. Now that we have all the bits we need, we can work out the Bayes factor. We’ll jump back to our formula: \\[\\mathcal{M}_H = \\int_{\\theta\\in\\Theta_H}\\mathcal{L}_H(\\theta|\\mathbf{y})p(\\theta)d\\theta\\] This tells us that to work our the value for \\(\\mathcal{M}_H\\) we need to take the likelihood, multiply it by the prior, and then take the integral. And all this needs to be done over the range that the parameter takes. Effect sizes span the entire range of real numbers (i.e., \\(-\\infty\\) to \\(+\\infty\\)) so that’ll be our range for the alternative. For the null model, we’re only interested in one value of \\(\\delta\\) — when \\(\\delta\\) = 0. So we’ll use the range (0,0) instead. To do this, we just use the following syntax: H_M = data_model * hm_model(theta.range = c(minval,maxval)) Let’s perform the calculation for \\(\\mathcal{H}_0\\): # 11 # multiple the prior and likelihood for H_0 M0 = data_model * h0_model(theta.range = c(0,0)) M0 Object of class marginal Parameter range: from 0 to 0 Area under the curve (integral): 0.03386948 Prior function: function(theta) ifelse(theta == 0, 1, 0) Likelihood function: And then we’l do the same for \\(\\mathcal{H}_1\\): # 12 # multiple the prior and likelihood for H_1 M1 = data_model * h1_model(theta.range = c(-Inf, Inf)) M1 Object of class marginal Parameter range: from -Inf to Inf Area under the curve (integral): 0.033066 Prior function: function(theta) dcauchy(x = theta, location = 0, scale = 1) Likelihood function: Finally to work out the Bayes factor we just need to take the integral of model 1 and model 2, and then take the ratio of those values. To get the integral of a model just use the following syntax: M_H$integral # 13 BF01 = M0$integral / M1$integral BF01 [1] 1.024299 Now let’s compare our home made BF to the store bought version. # 14 glue::glue(&quot;The store bought **BF** using the **Bayes factor** package is {round(tidy(store_bought_bf)$BF01,2)} The one we made at home is {round(BF01,2)}&quot;) The store bought BF using the Bayes factor package is 1.02 The one we made at home is 1.02 We can put everything together in a single code block so it’s easier to follow: # 15 # calculate observed parameter value delta = mean(g_m_2008_data) / sd(g_m_2008_data) # define liklihood data_model = t.lik(center = delta, df = length(g_m_2008_data) - 1) # define model / priors h0_model = function(theta.range){ list(func = function(theta) ifelse(theta == 0, 1, 0), theta.range = theta.range) } h1_model = function(theta.range){ list(func = function(theta) dcauchy(x = theta, location = 0, scale = 1), theta.range = theta.range) } # multiply the prior and likelihoods and priors M0 = data_model * h0_model(theta.range = c(0,0)) M1 = data_model * h1_model(theta.range = c(-Inf, Inf)) # take the intergral and divide BF01 = M0$integral / M1$integral glue::glue(&quot;B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01,2)} B&lt;sub&gt;10&lt;/sub&gt; = {round(1/BF01,2)}&quot;) B01 = 1.02 B10 = 0.98 We can also work through the second example provided in Rouder et al (2009). Again, this deals with data from Grider and Malmberg (2008). This time, however, we’ll work directly from the summary stats as an example. Rouder et al (2009) give the two condition means as 0.76 and 0.79, and they report the t statistic as 2.03. For the Bayes factor, they give a BF of \\(\\frac{\\mathcal{H}_0}{\\mathcal{H}_1}\\) of 1.56. # 16 # calculate observed parameter value # effect sizes are just t / sqrt(N) t_value = 2.03 N = 80 delta = t_value / sqrt(N) # define liklihood data_model = t.lik(center = delta, df = N - 1) # define model / priors h0_model = function(theta.range){ list(func = function(theta) ifelse(theta == 0, 1, 0), theta.range = theta.range) } h1_model = function(theta.range){ list(func = function(theta) dcauchy(x = theta, location = 0, scale = 1), theta.range = theta.range) } # multiply the prior and likelihoods M0 = data_model * h0_model(theta.range = c(0,0)) M1 = data_model * h1_model(theta.range = c(-Inf, Inf)) # take the intergral and divide BF01 = M0$integral / M1$integral glue::glue(&quot;B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01,2)} B&lt;sub&gt;10&lt;/sub&gt; = {round(1/BF01,2)}&quot;) B01 = 1.56 B10 = 0.64 Now that we’re making our own Bayes factors we don’t have to use the built in priors. We can make our own if we want to. Do you think Rouder et al’s choices are reasonable? Do you have any other suggestions? One possibility for an alternative prior is actually suggested by Rouder et al. They refer to it as the unit information prior. The unit information prior is just a standard normal distribution—that is, a normal distribution with a mean of 0 and a standard deviation of 1. # 17 # define the prior h1_unit_information = function(theta.range){ list(func = function(theta) dnorm(x = theta, mean = 0, sd = 1), theta.range = theta.range) } # plot it over a reasonable range of theta theta.range = seq(-10,10,.01) # then make a tibble with the data for the plot tibble(theta = theta.range, p = h1_unit_information(theta.range)$func(theta.range)) %&gt;% ggplot(aes(x = theta, y = p)) + geom_line() + scale_x_continuous(name = &quot;δ&quot;) + scale_y_continuous(name = &quot;p(δ)&quot;) + theme_minimal(12) If you compare the plot above to the Cauchy prior you’ll notice that the drop off is a lot steeper. This choice of prior seems to do a better job of actually reflecting our beliefs about the reasonable range of effect sizes in psychology in general. There’s no appreciable mass at anything beyond 2.5, unlike the Cauchy which still had a lot of weight beyond 5. Let’s compute a Bayes factor using this new model. We don’t have to re-specify the likelihood or the null model, because we’ll just re-use everything from the previous example. # 18 M1_unit_info = data_model * h1_unit_information(theta.range = c(-Inf, Inf)) BF01 = M0$integral / M1_unit_info$integral glue::glue(&quot;B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01,2)} B&lt;sub&gt;10&lt;/sub&gt; = {round(1/BF01,2)}&quot;) B01 = 1.21 B10 = 0.83 We can see that the two priors yielded numerically different results. But they’re not so different as to warrant different conclusions. We can try a range of reasonable priors to see how our conclusions change. But we can also try completely unreasonable priors. For example, Let’s say that our alternative model consists of only a single value—the observed value. # 19 h1_psychic = function(theta.range){ list(func = function(theta) ifelse(theta == delta, 1, 0) , theta.range = theta.range) } M1_psychic = data_model * h1_psychic(theta.range = c(delta, delta)) BF01 = M0$integral / M1_psychic$integral glue::glue(&quot;B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01,2)} B&lt;sub&gt;10&lt;/sub&gt; = {round(1/BF01,2)}&quot;) B01 = 0.13 B10 = 7.55 Or alternatively, we might set our alternative to equally weight all values of \\(\\delta\\) from -100 to +100. Again, this is completely unreasonable and it’s just done as an illustration. # 20 h1_uniform = function(theta.range){ list(func = function(theta) dunif(x = theta, min = -100, max = 100) , theta.range = theta.range) } M1_uniform = data_model * h1_uniform(theta.range = c(-100, 100)) BF01 = M0$integral / M1_uniform$integral glue::glue(&quot;B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01,2)} B&lt;sub&gt;10&lt;/sub&gt; = {round(1/BF01,2)}&quot;) B01 = 97.42 B10 = 0.01 Rouder et al’s approach is to place priors on effect size. Effect sizes might be an attractive choice of parametrisation when we’re trying to come up with priors that are widely applicable. But when we’re thinking of scientific theories or reasoning about effects, it’s often easier to do it in terms of raw effects—that is, what is the difference in milliseconds or in accuracy. Since we’re building our own BFs we can build models for raw effects too. "],
["computing-bayes-factors-part-ii.html", "Chapter 7 Computing Bayes factors Part II 7.1 Priors on raw effects", " Chapter 7 Computing Bayes factors Part II 7.1 Priors on raw effects To explore this, we’re going to look at a few examples from Dienes (2014). Dienes (2014) presents an example of Dienes et al (2012) where it was predicted that negative mood would reduce a certain type of learning. Participants performed a two-alternative forced choice task where chance was 50%. Learning was measured in a neutral condition and a negative mood condition. It was found that in the neutral condition accuracy was 70%. Therefore, the theory that negative mood reduces learning would predict that in the negative mood condition accuracy would be somewhere between 50% (i.e., chance) and 70%. It happened to be the case that in the negative mood condition accuracy was 65%. How can one analyse these data? One option is to do a t-test. It so happened that a t-test produced the following result: t(50) = 0.5, p = .62. Another option would be a Bayes factor analysis. To do this we’ll need 4 things: A parameter to make inferences about A likelihood function relating our data to likely values of the parameter A model of \\(\\mathcal{H}_0\\) A model of \\(\\mathcal{H}_1\\) For this example, we’ll make inferences about the mean difference in accuracy. For our likelihood function, we’ll choose either a normal distribution or a scaled and shifted t distribution. For our \\(\\mathcal{H}_0\\), we’ll keep it simple and choose a point null—that is, mean difference = 0. For our \\(\\mathcal{H}_1\\), however, we’re actually going to try represent the predictions of the theory. So to recap, the theory predicts that in the negative condition, accuracy will be somewhere between 50-70% (i.e., more than chance but less than in the neutral condition). In terms of the mean difference between conditions, this means that the theory predicts a mean difference between 0% and 20%. We’ll follow Dienes (2014) and represent this prediction with the Uniform prior between 0% and 20% (however, you might want to think about other ways to represent this). So let’s put the parts together. # 1 # to define the likelihood we&#39;ll need the mean and the standard error negative_mood_acc = 65 neutral_acc = 70 mean_diff = abs(negative_mood_acc - neutral_acc) t_stat = 0.5 se = mean_diff / t_stat # 2 # define the likelihood # we&#39;ll use a normal likelihood # another option could be to use the scaled shifted t distribution # but we&#39;ll use it because Dienes (2014) uses a normal data_model1 = norm.lik(center = mean_diff, scale = se) data_model2 = scaled.shifted.t.lik(center = mean_diff, scale = se, df = 50) norm = plot(data_model1, theta = seq(-40,40,.1)) + geom_vline(xintercept = mean_diff, linetype = 2) + scale_x_continuous(name = &quot;mean difference&quot;, breaks = c(seq(-40,40,20),mean_diff)) + theme_minimal(12) + labs(title = &quot;normal&quot;) scaled_t = plot(data_model2, theta = seq(-40,40,.1)) + geom_vline(xintercept = mean_diff, linetype = 2) + scale_x_continuous(name = &quot;mean difference&quot;, breaks = c(seq(-40,40,20),mean_diff)) + theme_minimal(12) + labs(title = &quot;shifted scaled t&quot;) norm / scaled_t + plot_annotation(tag_levels = &quot;A&quot;) data_model = data_model1 # 3 # define model / priors h0_model = function(theta.range){ list(func = function(theta) ifelse(theta == 0, 1, 0), # a point null with mean diff = 0 theta.range = theta.range) } h1_model = function(theta.range){ list(func = function(theta) dunif(x = theta, min = 0, max = 20), # uniform prior from 0 to 20 theta.range = theta.range) } # plot the h1 prior theta.range = seq(-10,30,.1) # from -10 to +30 in steps 0.1 # then make a tibble with the data for the plot tibble(theta = theta.range, p = h1_model(theta.range)$func(theta.range)) %&gt;% ggplot(aes(x = theta, y = p)) + geom_line() + scale_x_continuous(name = &quot;mean difference&quot;) + scale_y_continuous(name = &quot;p(mean difference)&quot;) + theme_minimal(12) # 4 # multiply the prior and likelihoods and priors M0 = data_model * h0_model(theta.range = c(0,0)) M1 = data_model * h1_model(theta.range = c(0, Inf)) BF10 = M1$integral / M0$integral glue::glue(&quot;B&lt;sub&gt;10&lt;/sub&gt; = {round(BF10,2)}&quot;) B10 = 0.89 Now that we have our BF as a number, we’ll convert that into a verbal label using the categories for Wagenmakers et al (2017). #5 bfsay(BF10, numerator = &quot;M1&quot;, denominator = &quot;M0&quot;) Using the levels from Wagenmakers et al (2017; https://doi.org/10.3758/s13423-017-1323-7) A BF of 0.887225881943733 indicates: Anecdotal evidence for M0 So we can see that once we take into account the predictions of the theory, our difference between conditions doesn’t really discriminate between \\(\\mathcal{H}_0\\) and \\(\\mathcal{H}_1\\). We did find some evidence in favour of \\(\\mathcal{H}_0\\), but it wasn’t very much at all. Dienes extends the example a bit to show how BFs and p-values will sometimes lead to different conclusions. It’s worth examining the example to understand why this is the case. It’ll help us understand both BFs and p-values better. Dienes asks to consider two examples: A mean difference of 1 with a standard error of 10 A mean difference of 1 with a standard error of 1 He goes on to calculate p values and BFs (using the above model1) for both examples. We’ll do the same, and examine the results. 1A key point here is that the exact same priors are used for both examples. We might want to question whether this is reasonable. # 6 # Example 1 (t test) mean_diff = 1 se.1 = 10 df = 50 t_stat.1 = mean_diff / se.1 p_value.1 = 2 * pt(t_stat.1, df,lower.tail = F) glue::glue(&quot;For Example 1, a standard *t* test gives the result: *t*({df}) = {round(t_stat.1,2)}, *p* = {round(p_value.1,2)}&quot;) For Example 1, a standard t test gives the result: t(50) = 0.1, p = 0.92 # 7 # Example 1 with BFs # We&#39;ll use the same priors as before so we just have to change the likelihood (aka data mode) data_model.1 = norm.lik(center = mean_diff, scale = se.1) M0 = data_model.1 * h0_model(theta.range = c(0,0)) M1 = data_model.1 * h1_model(theta.range = c(0, Inf)) BF01.1 = M0$integral / M1$integral glue::glue(&quot;For example 1, the BF analysis gives B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01.1,2)}&quot;) For example 1, the BF analysis gives B01 = 1.55 bfsay(BF01.1,&quot;M0&quot;,&quot;M1&quot;) Using the levels from Wagenmakers et al (2017; https://doi.org/10.3758/s13423-017-1323-7) A BF of 1.55259053332338 indicates: Anecdotal evidence for M0 # 7 # Example 2 (t test) mean_diff = 1 se.2 = 1 df = 50 t_stat.2 = mean_diff / se.2 p_value.2 = 2 * pt(t_stat.2, df,lower.tail = F) glue::glue(&quot;For Example 2, a standard *t* test gives the result: *t*({df}) = {round(t_stat.2,2)}, *p* = {round(p_value.2,2)}&quot;) For Example 2, a standard t test gives the result: t(50) = 1, p = 0.32 # 8 # Example 2 with BFs # We&#39;ll use the same priors as before so we just have to change the likelihood (aka data mode) data_model.2 = norm.lik(center = mean_diff, scale = se.2) M0 = data_model.2 * h0_model(theta.range = c(0,0)) M1 = data_model.2 * h1_model(theta.range = c(0, Inf)) BF01.2 = M0$integral / M1$integral glue::glue(&quot;For example 2, the BF analysis gives B&lt;sub&gt;01&lt;/sub&gt; = {round(BF01.2,2)}&quot;) For example 2, the BF analysis gives B01 = 5.75 bfsay(BF01.2,&quot;M0&quot;,&quot;M1&quot;) Using the levels from Wagenmakers et al (2017; https://doi.org/10.3758/s13423-017-1323-7) A BF of 5.75200016141882 indicates: Moderate evidence for M0 Understanding the difference between these two inferences highlights how the two kinds of analysis ask different questions. Remember, p values ask about whether data are surprising given a particular parameter value. In contrast, BFs ask about plausible values of a parameter. We can highlight this different by plotting, on the same figure, the range of unsurprising data values and the likely values of the parameter given our data. # 9 # Example 1 # First get the unsurprising range of data values # work out the range of unsurprising values in t units t_range.1 = qt(.975, df = 50, ncp = 0) # unscale this so that it&#39;s in original units mean_range.1 = t_range.1 * se.1 plot(data_model.1, theta = seq(-30,30,.1)) + ylim(c(-.02,.4)) + geom_errorbarh(mapping = aes(xmin = -mean_range.1, xmax = mean_range.1, y = 0), height = .02) + geom_vline(xintercept = mean_diff, linetype = 2 ) + theme_minimal(12) + labs(subtitle = glue::glue(&quot;p = {round(p_value.1,2)}; BF01 = {round(BF01.1,2)}&quot;), x = &quot;mean difference&quot;, y = &quot;&quot;) # 10 # Example 2 # First get the unsurprising range of data values # work out the range of unsurprising values in t units t_range.2 = qt(.975, df = 50, ncp = 0) # unscale this so that it&#39;s in original units mean_range.2 = t_range.2 * se.2 plot(data_model.2, theta = seq(-30,30,.1)) + ylim(c(-.02,.4)) + geom_errorbarh(mapping = aes(xmin = -mean_range.2, xmax = mean_range.2, y = 0), height = .02) + geom_vline(xintercept = mean_diff, linetype = 2 ) + theme_minimal(12) + labs(subtitle = glue::glue(&quot;p = {round(p_value.2,2)}; BF01 = {round(BF01.2,2)}&quot;), x = &quot;mean difference&quot;, y = &quot;&quot;) In the plots we can see that in example 1, a very wide range of values of the mean difference could plausibly produce the value of the mean difference we actually observed. In example 2, however, the range of plausible values for the mean difference is very small. In example 1, the range of values includes values that are both very big and very small. That is, data show that we’re very uncertain about the actual value of the mean difference with the likelihood being wide and spread out; therefore, the BF value does allow us to make any strong conclusions either way about which model is best supported. In example 2, however, the range is narrow and all the values are very close to 0 (our null value)—that is, the likelihood is highly peaked near 0. The frequentist inference asks something completely different. We set the parameter to 0, and then generate data based on this. We compare this generated data with our actual data and ask if it’s surprising or not. From the first plot, we see that the “unsurprising” range is very large. Our data lies somewhere near the middle of the range. In the second figure, the range is very narrow. As a result, our data now lies right near the edge of the range. Again, because this is not a class on frequentist inference I won’t give details on what I think the correct frequentist analysis for these example should be. But these examples are illustrative because they employ the common way most people would analyse this kind data. "]
]
